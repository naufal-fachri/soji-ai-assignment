{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66b00eb9",
   "metadata": {},
   "source": [
    "# ðŸ›« Soji AI â€” Notebook Setup Guide\n",
    "\n",
    "This notebook provides step-by-step setup for running the AD Recognition pipeline interactively.\n",
    "\n",
    "> âš ï¸ **Important:** Choose **only one** section below based on your hardware. Running both will cause dependency conflicts.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Step 0: Check Your Hardware\n",
    "\n",
    "Run this cell first to detect your environment:\n",
    "\n",
    "```python\n",
    "import subprocess\n",
    "\n",
    "def check_gpu():\n",
    "    try:\n",
    "        result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… NVIDIA GPU detected!\")\n",
    "            print(result.stdout)\n",
    "            print(\"ðŸ‘‰ Follow: (GPU Setup)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ No NVIDIA GPU found\")\n",
    "            print(\"ðŸ‘‰ Follow: (CPU Setup) (âš ï¸ Note: the ocr process will take a while since using CPU)\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ nvidia-smi not found â€” no GPU available\")\n",
    "        print(\"ðŸ‘‰ Follow: (CPU Setup) (âš ï¸ Note: the ocr process will take a while since using CPU)\")\n",
    "        return False\n",
    "\n",
    "HAS_GPU = check_gpu()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## GPU Setup\n",
    "\n",
    "> **Requirements:** NVIDIA GPU with CUDA 13.0+ drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12306f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NVIDIA GPU detected!\n",
      "Sat Feb 21 04:40:35 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 590.57                 Driver Version: 591.86         CUDA Version: 13.1     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   42C    P0             20W /  100W |       0MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "ðŸ‘‰ Follow: (GPU Setup)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_gpu():\n",
    "    try:\n",
    "        result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… NVIDIA GPU detected!\")\n",
    "            print(result.stdout)\n",
    "            print(\"ðŸ‘‰ Follow: (GPU Setup)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ No NVIDIA GPU found\")\n",
    "            print(\"ðŸ‘‰ Follow: (CPU Setup) (âš ï¸ Note: the ocr process will take a while since using CPU)\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ nvidia-smi not found â€” no GPU available\")\n",
    "        print(\"ðŸ‘‰ Follow: (CPU Setup) (âš ï¸ Note: the ocr process will take a while since using CPU)\")\n",
    "        return False\n",
    "\n",
    "HAS_GPU = check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49f3ed",
   "metadata": {},
   "source": [
    "# Airworthiness Directive (AD) Applicability Parser\n",
    "\n",
    "Determining whether an aircraft is affected by an Airworthiness Directive (AD) requires engineers to manually cross-reference each AD document against fleet data â€” checking aircraft model, MSN, embodied modifications, and incorporated service bulletins. This process is time-consuming, error-prone, and does not scale as the number of ADs and fleet size grows.\n",
    "\n",
    "A key challenge is that **AD document layouts are not standardized and evolve over time**. Different issuing authorities (EASA, FAA, etc.) use varying formats, and even within the same authority, the structure changes across revisions. This makes traditional rule-based extraction using regex or template matching impractical â€” any hard-coded parsing logic would break as soon as the layout shifts, requiring constant maintenance with no guarantee of reliability.\n",
    "\n",
    "## Proposed Approaches\n",
    "\n",
    "To address this, we leverage LLMs for the extraction layer, as they can interpret unstructured regulatory text regardless of layout changes. Two implementation approaches are proposed:\n",
    "\n",
    "1. **Full LLM Extraction (Multimodal)** â€” The AD document (PDF) is sent directly to a multimodal LLM that processes both text and visual layout. This is simpler to implement and preserves the original document structure, including tables and formatting that may carry semantic meaning.\n",
    "\n",
    "2. **Local OCR + LLM (Text-Only)** â€” The document is first processed through a local OCR pipeline (e.g., PaddleOCR) to extract raw text, which is then fed to a text-only LLM for structured extraction. This offers more control over preprocessing, reduces multimodal API costs, and can run partially offline.\n",
    "\n",
    "Both approaches output structured JSON conforming to a Pydantic schema that captures the full AD structure: applicable models, MSN constraints, modification/SB exclusions, aircraft group definitions, and every required action paragraph with its compliance deadlines.\n",
    "\n",
    "## Applicability Engine\n",
    "\n",
    "Once parsed into structured JSON, a deterministic rule-based engine evaluates each aircraft against the extracted data through three sequential checks: **(1) Model Check** â€” is the aircraft's model listed in the AD's applicability; **(2) MSN Check** â€” does the serial number satisfy the AD's MSN constraints (all-MSN, ranges, specific lists, exclusions); **(3) Modification/SB Exclusion Check** â€” has the aircraft already embodied a modification or SB that exempts it. The output is an augmented fleet DataFrame with status indicators: `âœ… Affected`, `âŒ Not applicable`, or `âŒ Not Affected` (exempted).\n",
    "\n",
    "## Why This Architecture\n",
    "\n",
    "The LLM handles what regex cannot â€” understanding unstructured, evolving document formats â€” while the applicability logic remains fully deterministic and auditable. The \"decision\" layer never hallucinates; it only operates on structured data that can be reviewed before any determination is made. This separation ensures traceability, which is critical for aviation regulatory compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db6f337",
   "metadata": {},
   "source": [
    "## Core Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a953e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## System Prompt\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an aviation regulatory document parser specialized in Airworthiness Directives (ADs).\n",
    "Extract structured applicability and compliance information from the provided AD document.\n",
    "\n",
    "EXTRACTION RULES:\n",
    "- Extract only information explicitly stated in the document. Never infer or assume.\n",
    "- Preserve all identifiers verbatim (model names, SB numbers, mod numbers, MSNs).\n",
    "- If a field has no corresponding information in the document, set it to null.\n",
    "- Output valid JSON only. No markdown, no explanation, no commentary.\n",
    "\n",
    "CRITICAL DISTINCTIONS:\n",
    "- Airbus modification numbers (e.g. \"mod 24591\") â†’ always go in modification_constraints. Never in sb_constraints.\n",
    "- Service Bulletin identifiers (e.g. \"A320-57-1089\") â†’ always go in sb_constraints. Never in modification_constraints.\n",
    "- If the AD states \"all MSN\" or \"all manufacturer serial numbers\", always set MSNConstraint(all=True, excluded=False). Never leave msn_constraints null when MSN applicability is mentioned.\n",
    "- When multiple compliance limits use \"whichever occurs first\", list each as a separate ComplianceTime entry.\n",
    "- Recurring intervals (\"thereafter, at intervals not exceeding...\") â†’ is_interval=True.\n",
    "- One-time thresholds (\"before exceeding...\") â†’ is_interval=False.\n",
    "\n",
    "OUTPUT: Valid JSON strictly following the provided schema.\n",
    "\"\"\"\n",
    "\n",
    "## Schemas\n",
    "from enum import Enum\n",
    "from typing import Optional, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class TimeUnit(str, Enum):\n",
    "    FLIGHT_HOURS = \"flight_hours\"\n",
    "    FLIGHT_CYCLES = \"flight_cycles\"\n",
    "    DAYS = \"days\"\n",
    "    MONTHS = \"months\"\n",
    "    YEARS = \"years\"\n",
    "    CALENDAR_DATE = \"calendar_date\"\n",
    "\n",
    "\n",
    "class NumericRange(BaseModel):\n",
    "    start: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Lower bound of the MSN range (inclusive by default). \"\n",
    "            \"Set to None if there is no lower bound.\"\n",
    "        )\n",
    "    )\n",
    "    end: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Upper bound of the MSN range (inclusive by default). \"\n",
    "            \"Set to None if there is no upper bound.\"\n",
    "        )\n",
    "    )\n",
    "    inclusive_start: bool = Field(\n",
    "        default=True,\n",
    "        description=\"True means >= (greater than or equal to start). False means > (strictly greater than).\"\n",
    "    )\n",
    "    inclusive_end: bool = Field(\n",
    "        default=True,\n",
    "        description=\"True means <= (less than or equal to end). False means < (strictly less than).\"\n",
    "    )\n",
    "\n",
    "\n",
    "class MSNConstraint(BaseModel):\n",
    "    all: Optional[bool] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Set to True when the AD explicitly states 'all manufacturer serial numbers (MSN)' or 'all MSN'. \"\n",
    "            \"IMPORTANT: Never leave this None when the AD explicitly uses the word 'all' for MSN applicability â€” \"\n",
    "            \"even if other exclusions apply, the 'all' inclusion must still be captured here. \"\n",
    "            \"Leave None only when applicability is defined purely by a specific range or list.\"\n",
    "        )\n",
    "    )\n",
    "    range: Optional[NumericRange] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"A continuous numeric range of MSNs this constraint covers. \"\n",
    "            \"Use when the AD specifies a span like 'MSN 100 through MSN 500'. \"\n",
    "            \"Do not use together with specific_msns.\"\n",
    "        )\n",
    "    )\n",
    "    specific_msns: Optional[List[int]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"An explicit list of individual MSN integers this constraint covers. \"\n",
    "            \"Use when the AD names specific serial numbers, e.g. 'MSN 364 or MSN 385'. \"\n",
    "            \"Do not use together with range.\"\n",
    "        )\n",
    "    )\n",
    "    excluded: bool = Field(\n",
    "        default=False,\n",
    "        description=(\n",
    "            \"Set to True when these MSNs are EXCLUDED from applicability \"\n",
    "            \"(AD language like 'except MSN...', 'excluding MSN...'). \"\n",
    "            \"Set to False when these MSNs are positively INCLUDED in applicability. \"\n",
    "            \"Default is False (inclusion).\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class ModificationConstraint(BaseModel):\n",
    "    modification_id: str = Field(\n",
    "        description=(\n",
    "            \"The exact modification identifier as written in the AD. \"\n",
    "            \"Always an Airbus 'mod' number, e.g. 'mod 24591', 'mod 24977'. \"\n",
    "            \"IMPORTANT: Modification numbers are never Service Bulletins â€” \"\n",
    "            \"do not confuse with SB identifiers (e.g. 'A320-57-XXXX'). \"\n",
    "            \"Copy the identifier verbatim from the AD text.\"\n",
    "        )\n",
    "    )\n",
    "    embodied: Optional[bool] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"True = this modification IS embodied on the aircraft. \"\n",
    "            \"False = this modification is NOT embodied on the aircraft. \"\n",
    "            \"None = embodiment status is unspecified or not relevant to this constraint.\"\n",
    "        )\n",
    "    )\n",
    "    excluded: bool = Field(\n",
    "        default=False,\n",
    "        description=(\n",
    "            \"Set to True when aircraft WITH this modification embodied are EXCLUDED from applicability \"\n",
    "            \"(AD language like 'except those on which mod XXXXX has been embodied in production'). \"\n",
    "            \"Set to False when this modification is a positive inclusion condition. \"\n",
    "            \"Default is False (inclusion).\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class ServiceBulletinConstraint(BaseModel):\n",
    "    sb_identifier: str = Field(\n",
    "        description=(\n",
    "            \"The exact Service Bulletin identifier as written in the AD, \"\n",
    "            \"e.g. 'A320-57-1089', 'A320-57-1100'. \"\n",
    "            \"IMPORTANT: Only actual Airbus Service Bulletins belong here (format: 'AXXX-XX-XXXX'). \"\n",
    "            \"Airbus modification numbers ('mod XXXXX') must NEVER be placed here â€” \"\n",
    "            \"those belong exclusively in ModificationConstraint. \"\n",
    "            \"Copy the identifier verbatim from the AD text, without the 'SB' prefix.\"\n",
    "        )\n",
    "    )\n",
    "    revision: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"The revision qualifier for this SB constraint, exactly as stated in the AD. \"\n",
    "            \"Examples: 'Revision 04', 'any revision lower than Revision 04', 'Revision 03 or later'. \"\n",
    "            \"Leave None if no specific revision is mentioned and any revision applies.\"\n",
    "        )\n",
    "    )\n",
    "    incorporated: Optional[bool] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"True = this SB HAS been incorporated on the aircraft. \"\n",
    "            \"False = this SB has NOT been incorporated on the aircraft. \"\n",
    "            \"None = incorporation status is unspecified or not relevant to this constraint.\"\n",
    "        )\n",
    "    )\n",
    "    excluded: bool = Field(\n",
    "        default=False,\n",
    "        description=(\n",
    "            \"Set to True when aircraft on which this SB HAS been embodied are EXCLUDED from applicability \"\n",
    "            \"(AD language like 'except those on which SB XXXX has been embodied'). \"\n",
    "            \"Set to False when this SB is a positive inclusion or compliance condition. \"\n",
    "            \"Default is False (inclusion).\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class AircraftGroup(BaseModel):\n",
    "    group_id: str = Field(\n",
    "        description=(\n",
    "            \"The group label exactly as defined in the AD's Groups section. \"\n",
    "            \"Examples: 'Group 1', 'Group 2', 'Group A', 'Group B'. \"\n",
    "            \"Use verbatim from the AD â€” do not invent or rename groups.\"\n",
    "        )\n",
    "    )\n",
    "    models: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Aircraft model variants that belong to this group, \"\n",
    "            \"derived from the group definition. \"\n",
    "            \"Examples: ['A321-111', 'A321-112'] or ['A320']. \"\n",
    "            \"Leave None if the group definition does not restrict by model \"\n",
    "            \"(i.e. it applies to all models already listed in the top-level applicability).\"\n",
    "        )\n",
    "    )\n",
    "    msn_constraints: Optional[List[MSNConstraint]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"MSN-based constraints that define or restrict membership in this group. \"\n",
    "            \"Apply the same rules as top-level msn_constraints: \"\n",
    "            \"if the group definition says 'all MSN', populate with MSNConstraint(all=True, excluded=False). \"\n",
    "            \"If the group is defined by specific MSNs, list them in specific_msns. \"\n",
    "            \"Leave None only if MSN is not a factor in this group's definition.\"\n",
    "        )\n",
    "    )\n",
    "    modification_constraints: Optional[List[ModificationConstraint]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Modification-based constraints that define or exclude aircraft from this group. \"\n",
    "            \"Only use ModificationConstraint here â€” never mix with SB identifiers. \"\n",
    "            \"Examples: a group excluding aircraft with a specific mod embodied in production. \"\n",
    "            \"Leave None if modifications are not a factor in this group's definition.\"\n",
    "        )\n",
    "    )\n",
    "    sb_constraints: Optional[List[ServiceBulletinConstraint]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Service Bulletin constraints that define or exclude aircraft from this group. \"\n",
    "            \"Only use actual SB identifiers here â€” never use mod numbers. \"\n",
    "            \"Example: a group defined by aircraft on which a specific SB has NOT been embodied. \"\n",
    "            \"Leave None if SBs are not a factor in this group's definition.\"\n",
    "        )\n",
    "    )\n",
    "    description: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Free-text fallback for group membership logic that cannot be fully expressed \"\n",
    "            \"by the structured fields above. \"\n",
    "            \"Transcribe the exact defining sentence from the AD. \"\n",
    "            \"Always populate this field â€” it serves as a human-readable audit trail \"\n",
    "            \"even when structured fields are also populated.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class ComplianceTime(BaseModel):\n",
    "    value: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"The numeric value of this compliance time. Always a positive integer. \"\n",
    "            \"Examples: 37300 for '37 300 flight hours', 24 for '24 months', 90 for '90 days'. \"\n",
    "            \"Set to None only when a specific calendar_date is used instead of a relative time value.\"\n",
    "        )\n",
    "    )\n",
    "    unit: Optional[TimeUnit] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"The unit of measurement corresponding to value. \"\n",
    "            \"Must be one of the TimeUnit enum values. \"\n",
    "            \"Set to None only when calendar_date is used instead of value+unit.\"\n",
    "        )\n",
    "    )\n",
    "    reference: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"The reference point from which this time is measured, transcribed from the AD. \"\n",
    "            \"Examples: 'since first flight of the aeroplane', \"\n",
    "            \"'after the effective date of this AD', \"\n",
    "            \"'since the last inspection', \"\n",
    "            \"'from the effective date of this AD'. \"\n",
    "            \"Leave None only if no reference point is stated and the context is self-evident.\"\n",
    "        )\n",
    "    )\n",
    "    calendar_date: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"An absolute calendar deadline in ISO 8601 format (YYYY-MM-DD). \"\n",
    "            \"Use only when the AD specifies a hard date rather than a relative time window. \"\n",
    "            \"When populated, value and unit should be None. \"\n",
    "            \"Example: '2026-06-01' for 'before 01 June 2026'.\"\n",
    "        )\n",
    "    )\n",
    "    is_interval: bool = Field(\n",
    "        default=False,\n",
    "        description=(\n",
    "            \"Set to True for RECURRING intervals between repeated actions \"\n",
    "            \"(AD language like 'thereafter, at intervals not exceeding X FH'). \"\n",
    "            \"Set to False for one-time initial thresholds \"\n",
    "            \"(AD language like 'before exceeding X FH since first flight'). \"\n",
    "            \"Default is False.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class RequirementAction(BaseModel):\n",
    "    paragraph_id: str = Field(\n",
    "        description=(\n",
    "            \"The paragraph identifier exactly as numbered in the AD's Required Actions section. \"\n",
    "            \"Examples: '(1)', '(5)', '(8)', '(12)'. \"\n",
    "            \"Used to cross-reference paragraphs (e.g. corrective actions referencing their \"\n",
    "            \"triggering inspection paragraph).\"\n",
    "        )\n",
    "    )\n",
    "    action_type: str = Field(\n",
    "        description=(\n",
    "            \"The category of this required action. Use exactly one of the following values: \"\n",
    "            \"'inspection' â€” any DET, GVI, SDI, ESDI, or other inspection task; \"\n",
    "            \"'modification' â€” a structural, design, or configuration change to the aircraft; \"\n",
    "            \"'corrective_action' â€” a repair or follow-up action triggered by a finding during inspection; \"\n",
    "            \"'terminating_action' â€” an action whose accomplishment ends one or more repetitive requirements; \"\n",
    "            \"'prohibition' â€” an action that must NOT be accomplished (e.g. 'do not embody SB X below Rev Y'); \"\n",
    "            \"'clarification' â€” a paragraph that clarifies scope or interaction between other paragraphs \"\n",
    "            \"without itself requiring a physical action (e.g. 'accomplishment of paragraph X does not \"\n",
    "            \"terminate paragraph Y').\"\n",
    "        )\n",
    "    )\n",
    "    applies_to_groups: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"List of group IDs, exactly as defined in the AD's Groups section, \"\n",
    "            \"to which this requirement applies. \"\n",
    "            \"Examples: ['Group 1'], ['Group 1', 'Group 4']. \"\n",
    "            \"Leave None if the requirement is stated in terms of direct model references \"\n",
    "            \"rather than group labels, or if it applies implicitly to all groups \"\n",
    "            \"(e.g. clarification paragraphs).\"\n",
    "        )\n",
    "    )\n",
    "    applies_to_models: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Direct aircraft model references for requirements that do not use group labels. \"\n",
    "            \"Examples: ['A320-211', 'A320-212']. \"\n",
    "            \"Leave None when applies_to_groups is populated â€” do not duplicate the same \"\n",
    "            \"applicability in both fields.\"\n",
    "        )\n",
    "    )\n",
    "    additional_applicability_condition: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Any further condition within the stated group or model scope that narrows \"\n",
    "            \"which aircraft this paragraph applies to, transcribed verbatim from the AD. \"\n",
    "            \"Use when the paragraph adds a qualifier beyond the group definition itself. \"\n",
    "            \"Examples: \"\n",
    "            \"'except aeroplanes modified in accordance with the instructions of Airbus SB A320-57-1100', \"\n",
    "            \"'having embodied SB A320-57-1089 at any revision lower than Revision 04 (for Group 4 aeroplanes)'. \"\n",
    "            \"Leave None if no additional condition is stated.\"\n",
    "        )\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=(\n",
    "            \"A concise, self-contained human-readable summary of what action must be performed. \"\n",
    "            \"Include: the inspection method or action type (e.g. DET, GVI, modification), \"\n",
    "            \"the area or component involved, and the reference document(s) to follow. \"\n",
    "            \"Write in plain language suitable for a maintenance engineer to understand at a glance. \"\n",
    "            \"Example: 'Accomplish a detailed inspection (DET) of the LH and RH wing inner rear spars \"\n",
    "            \"at the MLG anchorage fitting attachment holes, per SB A320-57-1101 Revision 04.'\"\n",
    "        )\n",
    "    )\n",
    "    compliance_times: Optional[List[ComplianceTime]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"One or more initial compliance thresholds by which this action must first be accomplished. \"\n",
    "            \"When the AD states multiple limits with 'whichever occurs first', \"\n",
    "            \"list each as a separate ComplianceTime entry â€” the whichever-first logic is implied \"\n",
    "            \"by multiple entries in this list. \"\n",
    "            \"Example: '37 300 FH or 20 000 FC whichever occurs first since first flight' â†’ \"\n",
    "            \"two ComplianceTime entries: one for 37300 FH and one for 20000 FC, \"\n",
    "            \"both with reference 'since first flight of the aeroplane' and is_interval=False. \"\n",
    "            \"Leave None for clarification paragraphs or terminating action notes with no time limit.\"\n",
    "        )\n",
    "    )\n",
    "    interval: Optional[List[ComplianceTime]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"One or more recurring intervals for repetitive requirements. \"\n",
    "            \"Populate only when the AD states 'thereafter, at intervals not exceeding...'. \"\n",
    "            \"As with compliance_times, list each limit as a separate ComplianceTime entry \"\n",
    "            \"when multiple limits apply with 'whichever occurs first'. \"\n",
    "            \"All entries must have is_interval=True. \"\n",
    "            \"Leave None for one-time actions (modifications, one-time inspections, corrective actions).\"\n",
    "        )\n",
    "    )\n",
    "    reference_documents: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"List of Airbus Service Bulletins or other technical documents whose instructions \"\n",
    "            \"must be followed to accomplish this action. \"\n",
    "            \"Include the revision where the AD specifies it. \"\n",
    "            \"Examples: ['SB A320-57-1101 Revision 04', 'SB A320-57-1256']. \"\n",
    "            \"Leave None for corrective actions where the repair instructions are obtained \"\n",
    "            \"from Airbus on a case-by-case basis, or for clarification paragraphs.\"\n",
    "        )\n",
    "    )\n",
    "    triggered_by_paragraph: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"For corrective_action paragraphs only: the paragraph_id of the inspection \"\n",
    "            \"or action that triggers this corrective action when discrepancies are found. \"\n",
    "            \"Example: '(1)' means this corrective action is triggered by findings during \"\n",
    "            \"the inspection required by paragraph (1). \"\n",
    "            \"Leave None for all non-corrective action types.\"\n",
    "        )\n",
    "    )\n",
    "    terminating_action_for: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"List of paragraph_ids whose repetitive requirements are permanently terminated \"\n",
    "            \"upon accomplishment of this action. \"\n",
    "            \"Example: ['(5)'] means completing this action ends the recurring inspections \"\n",
    "            \"required by paragraph (5) for that aircraft. \"\n",
    "            \"Leave None if this action has no terminating effect on other paragraphs. \"\n",
    "            \"Note: also set is_terminating_action=True when this field is populated.\"\n",
    "        )\n",
    "    )\n",
    "    is_terminating_action: bool = Field(\n",
    "        default=False,\n",
    "        description=(\n",
    "            \"Set to True if accomplishing this action permanently terminates one or more \"\n",
    "            \"repetitive requirements in this AD. \"\n",
    "            \"Must be True whenever terminating_action_for is populated. \"\n",
    "            \"Default is False.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class ADApplicabilityExtraction(BaseModel):\n",
    "    ad_number: str = Field(\n",
    "        description=(\n",
    "            \"The full AD identifier including any revision suffix, exactly as it appears in the AD header. \"\n",
    "            \"Examples: '2025-0254R1', '2023-0041', 'AD 2021-23-10'. \"\n",
    "            \"Never omit the revision suffix if present.\"\n",
    "        )\n",
    "    )\n",
    "    issuing_authority: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"The aviation authority that issued this AD. \"\n",
    "            \"Examples: 'EASA', 'FAA', 'TCCA', 'CASA'. \"\n",
    "            \"Taken from the AD header or introductory paragraph.\"\n",
    "        )\n",
    "    )\n",
    "    effective_date: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"The effective date of this AD (or its most recent revision) in ISO 8601 format (YYYY-MM-DD). \"\n",
    "            \"If multiple dates are listed (original issue and revision), use the revision's effective date. \"\n",
    "            \"Example: '2025-12-08'.\"\n",
    "        )\n",
    "    )\n",
    "    revision: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"The revision label of this AD exactly as stated in the document. \"\n",
    "            \"Examples: 'Revision 01', 'R1', 'Amendment 2'. \"\n",
    "            \"Leave None for original issue (no revision).\"\n",
    "        )\n",
    "    )\n",
    "    supersedes: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"List of AD identifiers that this AD supersedes, replaces, or revises, \"\n",
    "            \"taken from the Revision field or the Reason section. \"\n",
    "            \"Include all superseded ADs, not just the immediate predecessor. \"\n",
    "            \"Examples: ['2025-0254', '2007-0162', '2014-0169']. \"\n",
    "            \"Leave None if this is a first-issue AD that supersedes nothing.\"\n",
    "        )\n",
    "    )\n",
    "    models: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Complete list of every aircraft model variant explicitly named in the \"\n",
    "            \"Applicability section of the AD. \"\n",
    "            \"List each variant as a separate string, exactly as written. \"\n",
    "            \"Examples: ['A320-211', 'A320-212', 'A320-214', 'A321-111', 'A321-112']. \"\n",
    "            \"Do not collapse variants (e.g. do not write 'A320' if the AD lists 'A320-211', 'A320-212' etc.).\"\n",
    "        )\n",
    "    )\n",
    "    msn_constraints: Optional[List[MSNConstraint]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Top-level MSN constraints covering the entire AD applicability, before any group scoping. \"\n",
    "            \"IMPORTANT â€” never leave this None when the AD mentions MSN applicability: \"\n",
    "            \"If the AD says 'all manufacturer serial numbers (MSN)' or 'all MSN', \"\n",
    "            \"always populate with at least one MSNConstraint(all=True, excluded=False). \"\n",
    "            \"If specific MSN ranges or numbers are excluded (e.g. 'except MSN 001 to 099'), \"\n",
    "            \"add a separate MSNConstraint with excluded=True for those. \"\n",
    "            \"Only leave None if the AD makes absolutely no reference to MSN applicability.\"\n",
    "        )\n",
    "    )\n",
    "    modification_constraints: Optional[List[ModificationConstraint]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Top-level Airbus modification constraints covering the entire AD applicability. \"\n",
    "            \"IMPORTANT: Only 'mod XXXXX' numbers belong here â€” never SB identifiers. \"\n",
    "            \"These are almost always exclusions: aircraft on which a specific mod has been \"\n",
    "            \"embodied in production are excluded from the AD's scope. \"\n",
    "            \"Capture each mod as a separate ModificationConstraint. \"\n",
    "            \"Example: 'except those on which Airbus mod 24591 has been embodied in production' â†’ \"\n",
    "            \"ModificationConstraint(modification_id='mod 24591', embodied=True, excluded=True). \"\n",
    "            \"Leave None only if no modification-based applicability constraints exist in this AD.\"\n",
    "        )\n",
    "    )\n",
    "    sb_constraints: Optional[List[ServiceBulletinConstraint]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Top-level Service Bulletin constraints covering the entire AD applicability. \"\n",
    "            \"IMPORTANT: Only actual Airbus SB identifiers (format 'AXXX-XX-XXXX') belong here. \"\n",
    "            \"Airbus modification numbers ('mod XXXXX') must NEVER be placed here â€” \"\n",
    "            \"those belong exclusively in modification_constraints. \"\n",
    "            \"These are typically SB-based exclusions, e.g. aircraft on which a specific SB \"\n",
    "            \"revision has been embodied are excluded from scope. \"\n",
    "            \"Example: 'except those on which SB A320-57-1089 at Revision 04 has been embodied' â†’ \"\n",
    "            \"ServiceBulletinConstraint(sb_identifier='A320-57-1089', revision='Revision 04', \"\n",
    "            \"incorporated=True, excluded=True). \"\n",
    "            \"Leave None only if no SB-based applicability constraints exist in this AD.\"\n",
    "        )\n",
    "    )\n",
    "    compliance_time: Optional[List[ComplianceTime]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Top-level summary of the most immediate compliance deadline(s) imposed by this AD as a whole. \"\n",
    "            \"The intent is to surface the AD's urgency at a glance, without requiring a consumer \"\n",
    "            \"to parse every RequirementAction. \"\n",
    "            \"Populate with the most restrictive (shortest) initial deadline across all requirements. \"\n",
    "            \"When the shortest deadline is expressed as 'X or Y whichever occurs first', \"\n",
    "            \"list both as separate ComplianceTime entries. \"\n",
    "            \"This field is a summary â€” full per-paragraph compliance times are still \"\n",
    "            \"captured in each RequirementAction.compliance_times. \"\n",
    "            \"Leave None only if this AD contains no time-limited requirements \"\n",
    "            \"(e.g. a purely prohibitive AD with no deadline).\"\n",
    "        )\n",
    "    )\n",
    "    groups: Optional[List[AircraftGroup]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Definitions of all aircraft groups declared in the AD's Groups section, \"\n",
    "            \"one AircraftGroup entry per defined group. \"\n",
    "            \"Groups are internal AD constructs that partition applicable aircraft for \"\n",
    "            \"the purpose of applying different requirements to different subsets. \"\n",
    "            \"Preserve the exact group labels and definitions from the AD. \"\n",
    "            \"Leave None only if the AD does not define any named groups.\"\n",
    "        )\n",
    "    )\n",
    "    requirements: Optional[List[RequirementAction]] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"Complete list of all required actions, one RequirementAction per numbered paragraph \"\n",
    "            \"in the AD's Required Actions section. \"\n",
    "            \"This is the primary output of the extraction. \"\n",
    "            \"Every paragraph must be captured â€” inspections, modifications, corrective actions, \"\n",
    "            \"prohibitions, terminating actions, and clarification notes alike. \"\n",
    "            \"Preserve paragraph numbering exactly as in the AD. \"\n",
    "            \"Leave None only if the AD contains no required actions (which should never occur \"\n",
    "            \"for a valid AD).\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "## Utils\n",
    "import re\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "\n",
    "def compare_to_ad(df: pd.DataFrame, ad_file_dict: dict) -> pd.DataFrame:\n",
    "\n",
    "    ad_columns = list(ad_file_dict.keys())\n",
    "    ad_rows = []\n",
    "\n",
    "    for _, item in df.iterrows():\n",
    "        model = str(item[\"aircraft_model\"])\n",
    "        msn = int(item[\"msn\"])\n",
    "\n",
    "        raw_mod = item[\"modifications_applied\"]\n",
    "        if pd.isna(raw_mod) or str(raw_mod).strip().lower() in (\"none\", \"n/a\", \"\"):\n",
    "            mods_applied = []\n",
    "        else:\n",
    "            mods_applied = [m.strip() for m in str(raw_mod).split(\",\")]\n",
    "\n",
    "        logger.info(\n",
    "            f\"ðŸ”Ž Checking AD status â€” model: {model}, MSN: {msn}, mods: {mods_applied}\"\n",
    "        )\n",
    "\n",
    "        ad_status_rows = []\n",
    "\n",
    "        for ad in ad_columns:\n",
    "            logger.debug(f\"   ðŸ“‹ Checking against: {ad}\")\n",
    "            ad_data = ad_file_dict[ad]\n",
    "\n",
    "            # --- Model check ---\n",
    "            model_status = any(model in m for m in ad_data[\"models\"])\n",
    "            if not model_status:\n",
    "                ad_status_rows.append(\"âŒ Not applicable\")\n",
    "                continue\n",
    "\n",
    "            # --- MSN check ---\n",
    "            msn_constraints = ad_data.get(\"msn_constraints\") or []\n",
    "\n",
    "            if not msn_constraints:\n",
    "                msn_status = True\n",
    "            else:\n",
    "                msn_status = False\n",
    "                for msn_constraint in msn_constraints:\n",
    "                    all_msn = msn_constraint.get(\"all\")\n",
    "                    range_data = msn_constraint.get(\"range\")\n",
    "                    specific = msn_constraint.get(\"specific_msns\")\n",
    "                    excluded = msn_constraint.get(\"excluded\", False)\n",
    "\n",
    "                    matched = False\n",
    "\n",
    "                    if all_msn:\n",
    "                        matched = True\n",
    "                    elif range_data:\n",
    "                        start = range_data.get(\"start\")\n",
    "                        end = range_data.get(\"end\")\n",
    "                        incl_start = range_data.get(\"inclusive_start\", True)\n",
    "                        incl_end = range_data.get(\"inclusive_end\", True)\n",
    "                        lower_ok = (msn >= start) if incl_start else (msn > start)\n",
    "                        upper_ok = (msn <= end) if incl_end else (msn < end)\n",
    "                        matched = lower_ok and upper_ok\n",
    "                    elif specific:\n",
    "                        matched = msn in specific\n",
    "\n",
    "                    if matched:\n",
    "                        msn_status = not excluded\n",
    "                        break\n",
    "\n",
    "            if not msn_status:\n",
    "                ad_status_rows.append(\"âŒ Not applicable\")\n",
    "                continue\n",
    "\n",
    "            # --- Modification / SB exclusion check ---\n",
    "            if not mods_applied:\n",
    "                ad_status_rows.append(\"âœ… Affected\")\n",
    "                continue\n",
    "\n",
    "            excluded_by_mod = False\n",
    "\n",
    "            for mod_applied in mods_applied:\n",
    "                if \"mod\" in mod_applied.lower():\n",
    "                    mod_constraints = ad_data.get(\"modification_constraints\") or []\n",
    "                    for mod_constraint in mod_constraints:\n",
    "                        mod_id = mod_constraint.get(\"modification_id\", \"\")\n",
    "                        is_excluded = mod_constraint.get(\"excluded\", False)\n",
    "                        if re.search(r\"\\b\" + re.escape(mod_id) + r\"\\b\", mod_applied):\n",
    "                            if is_excluded:\n",
    "                                excluded_by_mod = True\n",
    "                            break\n",
    "                else:\n",
    "                    sb_constraints = ad_data.get(\"sb_constraints\") or []\n",
    "                    for sb_constraint in sb_constraints:\n",
    "                        sb_id = sb_constraint.get(\"sb_identifier\", \"\")\n",
    "                        is_excluded = sb_constraint.get(\"excluded\", False)\n",
    "                        if re.search(r\"\\b\" + re.escape(sb_id) + r\"\\b\", mod_applied):\n",
    "                            if is_excluded:\n",
    "                                excluded_by_mod = True\n",
    "                            break\n",
    "\n",
    "                if excluded_by_mod:\n",
    "                    break\n",
    "\n",
    "            if excluded_by_mod:\n",
    "                ad_status_rows.append(\"âŒ Not Affected\")\n",
    "            else:\n",
    "                ad_status_rows.append(\"âœ… Affected\")\n",
    "\n",
    "        ad_rows.append(ad_status_rows)\n",
    "\n",
    "    ad_df = pd.DataFrame(ad_rows, columns=ad_columns)\n",
    "    combined_df = pd.concat([df, ad_df], axis=1)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d7386f",
   "metadata": {},
   "source": [
    "Component 1: LLM-Based AD Document Parser\n",
    "\n",
    "The first component uses a large language model (LLM) guided by a carefully engineered system prompt to extract structured data from unstructured AD documents. The system prompt enforces strict extraction rules â€” the model must only extract explicitly stated information, preserve all identifiers verbatim (model designations, Service Bulletin numbers, modification numbers, MSNs), and output valid JSON conforming to a predefined Pydantic schema. Critical distinctions are enforced at the prompt level: Airbus modification numbers (e.g., `mod 24591`) are always routed to `modification_constraints` and never confused with Service Bulletin identifiers (e.g., `A320-57-1089`), which are routed to `sb_constraints`. This separation is essential because modifications and service bulletins have fundamentally different implications for AD applicability.\n",
    "\n",
    "The output schema (`ADApplicabilityExtraction`) captures the full structure of an AD, including: the AD identifier and metadata (issuing authority, effective date, revision history), the complete list of applicable aircraft models and MSN constraints, modification and service bulletin exclusions, aircraft group definitions (which partition the fleet into subsets with different compliance requirements), and every numbered required action paragraph â€” each annotated with its action type, compliance deadlines, recurring intervals, reference documents, and cross-references to other paragraphs (e.g., corrective actions triggered by inspection findings, or terminating actions that end repetitive requirements).\n",
    "\n",
    "Component 2: Rule-Based Applicability Engine\n",
    "\n",
    "The second component (`compare_to_ad`) takes a fleet inventory DataFrame â€” containing each aircraft's model, MSN, and applied modifications â€” and evaluates it against the parsed AD data. For each aircraft-AD pair, the engine performs a three-stage check:\n",
    "\n",
    "1. **Model Check** â€” Verifies whether the aircraft's type certificate model appears in the AD's applicability list.\n",
    "2. **MSN Check** â€” Evaluates whether the aircraft's serial number falls within the AD's MSN constraints, supporting `all MSN` declarations, numeric ranges with configurable inclusivity bounds, specific MSN lists, and exclusion logic.\n",
    "3. **Modification/SB Exclusion Check** â€” Determines whether any modification or service bulletin already embodied on the aircraft exempts it from the AD's scope, using regex-based identifier matching against the parsed constraint data.\n",
    "\n",
    "The output is an augmented DataFrame where each AD column contains a status indicator: `âœ… Affected` (the aircraft is subject to the AD), `âŒ Not applicable` (the aircraft does not meet the AD's applicability criteria), or `âŒ Not Affected` (the aircraft originally fell within scope but is exempted by an already-embodied modification or service bulletin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34d51b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naufal/soji_ai/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[33mChecking connectivity to the model hosters, this may take a while. To bypass this check, set `PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK` to `True`.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Pipeline (With Full LLM)\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "from typing import Optional\n",
    "from loguru import logger\n",
    "from pydantic import BaseModel\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pdf2image import convert_from_bytes\n",
    "\n",
    "class ADRecognitionFullLLM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dpi: int,\n",
    "        llm_model: str,\n",
    "        llm_system_prompt: str,\n",
    "        llm_temperature: float,\n",
    "        llm_output_schema: type[BaseModel],\n",
    "        temp_dir: Optional[str] = None,\n",
    "    ):\n",
    "        self.dpi = dpi\n",
    "        self.llm_client = genai.Client(\n",
    "            api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "        )\n",
    "        self.llm_model = llm_model\n",
    "        self.llm_system_prompt = llm_system_prompt\n",
    "        self.llm_temperature = llm_temperature\n",
    "        self.llm_output_schema = llm_output_schema\n",
    "\n",
    "        if not temp_dir:\n",
    "            current_dir = os.getcwd()\n",
    "            self.temp_dir = os.path.join(current_dir, \"tmp/ad_recognition\")\n",
    "\n",
    "        else:\n",
    "            self.temp_dir = temp_dir\n",
    "            \n",
    "        os.makedirs(self.temp_dir, exist_ok=True)\n",
    "        self._run_dirs: list[str] = []  # track created run dirs for cleanup\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #  Helper: Derive AD label from filename\n",
    "    # ------------------------------------------------------------------ #\n",
    "    @staticmethod\n",
    "    def _label_from_path(pdf_path: str) -> str:\n",
    "        return os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #  Cleanup\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _cleanup_temp(self):\n",
    "        \"\"\"Remove all temporary run directories created during this session.\"\"\"\n",
    "        if not self._run_dirs:\n",
    "            return\n",
    "\n",
    "        logger.info(f\"ðŸ§¹ Cleaning up {len(self._run_dirs)} temp directories...\")\n",
    "        for run_dir in self._run_dirs:\n",
    "            try:\n",
    "                shutil.rmtree(run_dir)\n",
    "                logger.debug(f\"   ðŸ—‘ï¸  Removed: {run_dir}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"   âš ï¸  Failed to remove {run_dir}: {e}\")\n",
    "        self._run_dirs.clear()\n",
    "\n",
    "        # Remove parent temp dir if empty\n",
    "        try:\n",
    "            if os.path.exists(self.temp_dir) and not os.listdir(self.temp_dir):\n",
    "                os.rmdir(self.temp_dir)\n",
    "                logger.debug(f\"   ðŸ—‘ï¸  Removed empty temp dir: {self.temp_dir}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        logger.info(\"âœ… Cleanup complete\")\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #  Step 1: PDF -> Images\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _pdf_to_images(self, pdf_path: str, run_dir: str) -> list[str]:\n",
    "        logger.info(f\"ðŸ“„ Converting PDF to images: {pdf_path} (dpi={self.dpi})\")\n",
    "        imgs_dir = os.path.join(run_dir, \"pages\")\n",
    "        os.makedirs(imgs_dir, exist_ok=True)\n",
    "\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            img_paths = convert_from_bytes(\n",
    "                f.read(),\n",
    "                output_folder=imgs_dir,\n",
    "                fmt=\"png\",\n",
    "                paths_only=True,\n",
    "                dpi=self.dpi,\n",
    "            )\n",
    "        logger.info(f\"ðŸ–¼ï¸  Generated {len(img_paths)} page images\")\n",
    "        return img_paths\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #  Step 2: Prepare LLM messages\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _prepare_messages(self, img_paths: list[str]) -> list:\n",
    "        logger.info(f\"ðŸ“¦ Preparing {len(img_paths)} images for LLM...\")\n",
    "        messages = [\"Now, extract the following images!\"]\n",
    "        for img_path in img_paths:\n",
    "            logger.debug(f\"   ðŸ”— Encoding: {os.path.basename(img_path)}\")\n",
    "            with open(img_path, \"rb\") as f:\n",
    "                img_bytes = f.read()\n",
    "            messages.append(\n",
    "                types.Part.from_bytes(\n",
    "                    data=img_bytes,\n",
    "                    mime_type=\"image/png\",\n",
    "                )\n",
    "            )\n",
    "        logger.info(\"âœ… All images encoded and ready\")\n",
    "        return messages\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #  Step 3: Call Gemini for structured extraction\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _extract_with_llm(self, messages: list) -> dict:\n",
    "        logger.info(f\"ðŸ¤– Calling LLM model: {self.llm_model}\")\n",
    "\n",
    "        config = types.GenerateContentConfig(\n",
    "            system_instruction=self.llm_system_prompt,\n",
    "            temperature=self.llm_temperature,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_json_schema=self.llm_output_schema.model_json_schema(),\n",
    "        )\n",
    "\n",
    "        response = self.llm_client.models.generate_content(\n",
    "            model=self.llm_model,\n",
    "            config=config,\n",
    "            contents=messages,\n",
    "        )\n",
    "\n",
    "        parsed = self.llm_output_schema.model_validate_json(response.text)\n",
    "        logger.info(\"ðŸŽ¯ LLM extraction completed successfully\")\n",
    "        return parsed.model_dump()\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #  Step 4: Save extraction results\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def _save_extraction(self, data: dict, run_dir: str, label: str) -> str:\n",
    "        out_path = os.path.join(run_dir, f\"{label}_extraction.json\")\n",
    "        with open(out_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"ðŸ’¾ Saved extraction: {out_path}\")\n",
    "        return out_path\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #  Step 5: Extract a single AD PDF\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def extract_ad(self, pdf_path: str, label: Optional[str] = None) -> dict:\n",
    "        if label is None:\n",
    "            label = self._label_from_path(pdf_path)\n",
    "\n",
    "        run_id = uuid4().hex\n",
    "        run_dir = os.path.join(self.temp_dir, run_id)\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "        self._run_dirs.append(run_dir)\n",
    "        logger.info(f\"ðŸš€ [{label}] Starting extraction â€” run_id={run_id}\")\n",
    "\n",
    "        img_paths = self._pdf_to_images(pdf_path, run_dir)\n",
    "        messages = self._prepare_messages(img_paths)\n",
    "        extraction = self._extract_with_llm(messages)\n",
    "        self._save_extraction(extraction, run_dir, label)\n",
    "\n",
    "        logger.info(f\"âœ… [{label}] Extraction complete!\")\n",
    "        return extraction\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    #  Step 6: Full pipeline\n",
    "    # ------------------------------------------------------------------ #\n",
    "    def run_analysis(\n",
    "        self,\n",
    "        test_data_path: str,\n",
    "        ad_file_paths: list[str],\n",
    "        save_dir: str,\n",
    "        cleanup: bool = True,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Run the complete AD recognition and comparison pipeline.\n",
    "\n",
    "        Args:\n",
    "            test_data_path: Path to test CSV file.\n",
    "            ad_file_paths: List of AD PDF file paths to extract and compare.\n",
    "            save_dir: Directory to save final results.\n",
    "            cleanup: Whether to delete temp directories after saving results.\n",
    "\n",
    "        Returns:\n",
    "            Path to the saved results CSV.\n",
    "        \"\"\"\n",
    "        logger.info(\"ðŸ”°\" + \"=\" * 58)\n",
    "        logger.info(f\"ðŸ›« Starting AD Recognition Pipeline â€” {len(ad_file_paths)} AD(s)\")\n",
    "        logger.info(\"ðŸ”°\" + \"=\" * 58)\n",
    "\n",
    "        try:\n",
    "            # --- Extract all AD PDFs ---\n",
    "            ad_extractions: dict[str, dict] = {}\n",
    "            for i, pdf_path in enumerate(ad_file_paths, 1):\n",
    "                label = self._label_from_path(pdf_path)\n",
    "                logger.info(f\"ðŸ“‹ [{i}/{len(ad_file_paths)}] Processing: {label}\")\n",
    "                extraction = self.extract_ad(pdf_path, label=label)\n",
    "                ad_extractions[label] = extraction\n",
    "\n",
    "            # --- Load test data ---\n",
    "            logger.info(f\"ðŸ“Š Loading test data: {test_data_path}\")\n",
    "            test_data = pd.read_csv(test_data_path, sep=\",\")\n",
    "            logger.info(f\"ðŸ“ Test data shape: {test_data.shape}\")\n",
    "\n",
    "            # --- Compare ---\n",
    "            logger.info(f\"âš™ï¸  Running AD comparison against {len(ad_extractions)} AD(s)...\")\n",
    "            result_df = compare_to_ad(test_data, ad_file_dict=ad_extractions)\n",
    "            logger.info(f\"ðŸ Comparison done â€” {len(result_df)} rows classified\")\n",
    "\n",
    "            # --- Present results ---\n",
    "            print(\"========== RESULT ==========\")\n",
    "            print(result_df.to_markdown(index=False))\n",
    "            print(\"============================\")\n",
    "\n",
    "            # --- Save results ---\n",
    "            run_timestamp = datetime.now().strftime(\"%y%m%d\")\n",
    "            run_id = uuid4().hex[:8]\n",
    "            run_output_dir = os.path.join(save_dir, f\"{run_id}_{run_timestamp}\")\n",
    "            os.makedirs(run_output_dir, exist_ok=True)\n",
    "            logger.info(f\"ðŸ“ Run output directory: {run_output_dir}\")\n",
    "\n",
    "            result_path = os.path.join(run_output_dir, \"ad_classification_results.csv\")\n",
    "            result_df.to_csv(result_path, index=False)\n",
    "            logger.info(f\"ðŸ’¾ Results saved: {result_path}\")\n",
    "\n",
    "            extractions_path = os.path.join(run_output_dir, \"ad_extractions.json\")\n",
    "            with open(extractions_path, \"w\") as f:\n",
    "                json.dump(ad_extractions, f, indent=2, ensure_ascii=False)\n",
    "            logger.info(f\"ðŸ’¾ Extractions saved: {extractions_path}\")\n",
    "\n",
    "        finally:\n",
    "            if cleanup:\n",
    "                self._cleanup_temp()\n",
    "\n",
    "        logger.info(\"ðŸ”°\" + \"=\" * 58)\n",
    "        logger.info(\"ðŸŽ‰ Pipeline complete!\")\n",
    "        logger.info(\"ðŸ”°\" + \"=\" * 58)\n",
    "\n",
    "        return result_path\n",
    "    \n",
    "## Pipeline (With OCR + LLM (text only))\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "from typing import Optional, List, Dict, Any\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from loguru import logger\n",
    "from pydantic import BaseModel\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pdf2image import convert_from_bytes\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "class ADRecognitionOCR:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dpi: int,\n",
    "        llm_model: str,\n",
    "        llm_system_prompt: str,\n",
    "        llm_temperature: float,\n",
    "        llm_output_schema: type[BaseModel],\n",
    "        ocr_device: str = \"cpu\",\n",
    "        ocr_precision: str = \"fp32\",\n",
    "        ocr_det_model: str = \"PP-OCRv5_mobile_det\",\n",
    "        ocr_rec_model: str = \"PP-OCRv5_mobile_rec\",\n",
    "        y_threshold: float = 15.0,\n",
    "        save_ocr_viz: bool = True,\n",
    "        cpu_threads: int = 8,\n",
    "        temp_dir: Optional[str] = None,\n",
    "    ):\n",
    "        self.dpi = dpi\n",
    "        self.y_threshold = y_threshold\n",
    "        self.save_ocr_viz = save_ocr_viz\n",
    "\n",
    "        # --- LLM ---\n",
    "        self.llm_client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "        self.llm_model = llm_model\n",
    "        self.llm_system_prompt = llm_system_prompt\n",
    "        self.llm_temperature = llm_temperature\n",
    "        self.llm_output_schema = llm_output_schema\n",
    "\n",
    "        # --- OCR Engine ---\n",
    "        is_cpu = ocr_device.lower() == \"cpu\"\n",
    "\n",
    "        if is_cpu:\n",
    "            logger.info(f\"ðŸ”§ Initializing PaddleOCR engine on CPU with {cpu_threads} threads...\")\n",
    "            _precision = \"fp32\"\n",
    "            _enable_mkldnn = False\n",
    "        else:\n",
    "            logger.info(f\"ðŸ”§ Initializing PaddleOCR engine on {ocr_device}...\")\n",
    "            _precision = ocr_precision\n",
    "            _enable_mkldnn = True\n",
    "\n",
    "        self.ocr_engine = PaddleOCR(\n",
    "            use_doc_orientation_classify=False,\n",
    "            use_doc_unwarping=False,\n",
    "            use_textline_orientation=False,\n",
    "            device=ocr_device,\n",
    "            precision=_precision,\n",
    "            enable_mkldnn=_enable_mkldnn,\n",
    "            text_detection_model_name=ocr_det_model,\n",
    "            text_recognition_model_name=ocr_rec_model,\n",
    "            cpu_threads=cpu_threads if is_cpu else None,\n",
    "        )\n",
    "\n",
    "        if is_cpu:\n",
    "            logger.info(f\"âœ… PaddleOCR engine ready (CPU mode â€” {cpu_threads} threads, mkldnn=off, fp32)\")\n",
    "        else:\n",
    "            logger.info(f\"âœ… PaddleOCR engine ready ({ocr_device}, {ocr_precision})\")\n",
    "\n",
    "        # --- Temp dir ---\n",
    "        if not temp_dir:\n",
    "            self.temp_dir = os.path.join(os.getcwd(), \"tmp/ad_recognition_ocr\")\n",
    "        else:\n",
    "            self.temp_dir = temp_dir\n",
    "        os.makedirs(self.temp_dir, exist_ok=True)\n",
    "        self._run_dirs: list[str] = []\n",
    "\n",
    "    # ================================================================== #\n",
    "    #  Helpers\n",
    "    # ================================================================== #\n",
    "    @staticmethod\n",
    "    def _label_from_path(pdf_path: str) -> str:\n",
    "        return os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "    def _cleanup_temp(self):\n",
    "        \"\"\"Remove all temporary run directories created during this session.\"\"\"\n",
    "        if not self._run_dirs:\n",
    "            return\n",
    "\n",
    "        logger.info(f\"ðŸ§¹ Cleaning up {len(self._run_dirs)} temp directories...\")\n",
    "        for run_dir in self._run_dirs:\n",
    "            try:\n",
    "                shutil.rmtree(run_dir)\n",
    "                logger.debug(f\"   ðŸ—‘ï¸  Removed: {run_dir}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"   âš ï¸  Failed to remove {run_dir}: {e}\")\n",
    "        self._run_dirs.clear()\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(self.temp_dir) and not os.listdir(self.temp_dir):\n",
    "                os.rmdir(self.temp_dir)\n",
    "                logger.debug(f\"   ðŸ—‘ï¸  Removed empty temp dir: {self.temp_dir}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        logger.info(\"âœ… Cleanup complete\")\n",
    "\n",
    "    # ================================================================== #\n",
    "    #  Step 1: PDF -> Images\n",
    "    # ================================================================== #\n",
    "    def _pdf_to_images(self, pdf_path: str, run_dir: str) -> list[str]:\n",
    "        logger.info(f\"ðŸ“„ Converting PDF to images: {pdf_path} (dpi={self.dpi})\")\n",
    "        imgs_dir = os.path.join(run_dir, \"pages\")\n",
    "        os.makedirs(imgs_dir, exist_ok=True)\n",
    "\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            img_paths = convert_from_bytes(\n",
    "                f.read(),\n",
    "                output_folder=imgs_dir,\n",
    "                fmt=\"png\",\n",
    "                paths_only=True,\n",
    "                dpi=self.dpi,\n",
    "            )\n",
    "        logger.info(f\"ðŸ–¼ï¸  Generated {len(img_paths)} page images\")\n",
    "        return img_paths\n",
    "\n",
    "    # ================================================================== #\n",
    "    #  Step 2: OCR\n",
    "    # ================================================================== #\n",
    "    def _run_ocr(self, img_paths: list[str]) -> list[dict]:\n",
    "        logger.info(f\"ðŸ” Running OCR on {len(img_paths)} pages...\")\n",
    "        ocr_results = list(self.ocr_engine.predict(img_paths))\n",
    "        logger.info(f\"âœ… OCR complete â€” {len(ocr_results)} pages processed\")\n",
    "        return ocr_results\n",
    "\n",
    "    # ================================================================== #\n",
    "    #  Step 3: OCR Postprocessing (sort + full text)\n",
    "    # ================================================================== #\n",
    "    @staticmethod\n",
    "    def _sort_ocr_reading_order(\n",
    "        texts: List[str],\n",
    "        boxes: List[np.ndarray],\n",
    "        y_threshold: float = 15.0,\n",
    "    ) -> tuple[List[str], List[np.ndarray]]:\n",
    "        \"\"\"Sort OCR results in natural reading order (top-to-bottom, left-to-right).\"\"\"\n",
    "        if not texts:\n",
    "            return texts, boxes\n",
    "\n",
    "        coords = []\n",
    "        for i, box in enumerate(boxes):\n",
    "            box = np.array(box)\n",
    "            if box.shape == (4,):\n",
    "                x_left = box[0]\n",
    "                y_center = (box[1] + box[3]) / 2\n",
    "            elif box.shape == (4, 2):\n",
    "                x_left = box[:, 0].min()\n",
    "                y_center = box[:, 1].mean()\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected box shape: {box.shape}\")\n",
    "            coords.append((i, x_left, y_center))\n",
    "\n",
    "        coords.sort(key=lambda c: c[2])\n",
    "\n",
    "        lines = []\n",
    "        current_line = [coords[0]]\n",
    "        for item in coords[1:]:\n",
    "            if abs(item[2] - current_line[0][2]) <= y_threshold:\n",
    "                current_line.append(item)\n",
    "            else:\n",
    "                lines.append(current_line)\n",
    "                current_line = [item]\n",
    "        lines.append(current_line)\n",
    "\n",
    "        sorted_indices = []\n",
    "        for line in lines:\n",
    "            line.sort(key=lambda c: c[1])\n",
    "            sorted_indices.extend([item[0] for item in line])\n",
    "\n",
    "        sorted_texts = [texts[i] for i in sorted_indices]\n",
    "        sorted_boxes = [boxes[i] for i in sorted_indices]\n",
    "        return sorted_texts, sorted_boxes\n",
    "\n",
    "    def _get_full_text(self, ocr_results: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Convert OCR results to full text in reading order with page headers.\"\"\"\n",
    "        all_pages_text = []\n",
    "        total_pages = len(ocr_results)\n",
    "\n",
    "        for page_idx, page in enumerate(ocr_results):\n",
    "            texts = page.get(\"rec_texts\", [])\n",
    "            boxes = page.get(\"rec_boxes\", [])\n",
    "\n",
    "            if not texts:\n",
    "                continue\n",
    "\n",
    "            sorted_texts, sorted_boxes = self._sort_ocr_reading_order(\n",
    "                texts, boxes, self.y_threshold\n",
    "            )\n",
    "\n",
    "            coords = []\n",
    "            for i, box in enumerate(sorted_boxes):\n",
    "                box = np.array(box)\n",
    "                if box.shape == (4,):\n",
    "                    y_center = (box[1] + box[3]) / 2\n",
    "                else:\n",
    "                    y_center = box[:, 1].mean()\n",
    "                coords.append((i, y_center))\n",
    "\n",
    "            lines_text = []\n",
    "            current_line_texts = [sorted_texts[0]]\n",
    "            current_y = coords[0][1]\n",
    "\n",
    "            for idx in range(1, len(coords)):\n",
    "                if abs(coords[idx][1] - current_y) <= self.y_threshold:\n",
    "                    current_line_texts.append(sorted_texts[idx])\n",
    "                else:\n",
    "                    line = \" \".join(t for t in current_line_texts if t.strip())\n",
    "                    if line.strip():\n",
    "                        lines_text.append(line)\n",
    "                    current_line_texts = [sorted_texts[idx]]\n",
    "                    current_y = coords[idx][1]\n",
    "\n",
    "            line = \" \".join(t for t in current_line_texts if t.strip())\n",
    "            if line.strip():\n",
    "                lines_text.append(line)\n",
    "\n",
    "            page_num = page_idx + 1\n",
    "            header = f\"\\n{'='*60}\\n  PAGE {page_num} / {total_pages}\\n{'='*60}\\n\"\n",
    "            all_pages_text.append(header + \"\\n\".join(lines_text))\n",
    "\n",
    "        return \"\\n\".join(all_pages_text)\n",
    "\n",
    "    # ================================================================== #\n",
    "    #  Step 4: Draw OCR bbox visualizations\n",
    "    # ================================================================== #\n",
    "    @staticmethod\n",
    "    def _draw_ocr_bboxes(\n",
    "        image_path: str,\n",
    "        ocr_result: dict,\n",
    "        output_path: str,\n",
    "        use_polys: bool = True,\n",
    "        box_color: str = \"red\",\n",
    "        text_color: str = \"blue\",\n",
    "        show_text: bool = False,\n",
    "        font_size: int = 14,\n",
    "    ) -> None:\n",
    "        \"\"\"Draw OCR bounding boxes on the original image and save.\"\"\"\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        texts = ocr_result.get(\"rec_texts\", [])\n",
    "        polys = ocr_result.get(\"rec_polys\" if use_polys else \"rec_boxes\", [])\n",
    "\n",
    "        try:\n",
    "            font = ImageFont.truetype(\n",
    "                \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", font_size\n",
    "            )\n",
    "        except Exception:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        for i, poly in enumerate(polys):\n",
    "            poly = np.array(poly)\n",
    "\n",
    "            if poly.shape == (4,):\n",
    "                x_min, y_min, x_max, y_max = poly\n",
    "                draw.rectangle([x_min, y_min, x_max, y_max], outline=box_color, width=2)\n",
    "                text_pos = (x_min, y_min - font_size - 2)\n",
    "            elif poly.shape == (4, 2):\n",
    "                points = [tuple(p) for p in poly.astype(int)]\n",
    "                points.append(points[0])\n",
    "                draw.line(points, fill=box_color, width=2)\n",
    "                text_pos = (int(poly[:, 0].min()), int(poly[:, 1].min()) - font_size - 2)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if show_text and i < len(texts) and texts[i].strip():\n",
    "                draw.text(text_pos, texts[i], fill=text_color, font=font)\n",
    "\n",
    "        img.save(output_path)\n",
    "\n",
    "    def _save_ocr_visualizations(\n",
    "        self,\n",
    "        img_paths: list[str],\n",
    "        ocr_results: list[dict],\n",
    "        save_dir: str,\n",
    "        label: str,\n",
    "    ) -> list[str]:\n",
    "        \"\"\"Draw and save bbox visualizations for all pages.\"\"\"\n",
    "        viz_dir = os.path.join(save_dir, f\"{label}_ocr_viz\")\n",
    "        os.makedirs(viz_dir, exist_ok=True)\n",
    "        viz_paths = []\n",
    "\n",
    "        logger.info(f\"ðŸŽ¨ Drawing OCR visualizations for {len(img_paths)} pages...\")\n",
    "        for i, (img_path, ocr_result) in enumerate(zip(img_paths, ocr_results)):\n",
    "            viz_path = os.path.join(viz_dir, f\"page_{i+1}_ocr_viz.png\")\n",
    "            self._draw_ocr_bboxes(\n",
    "                image_path=img_path,\n",
    "                ocr_result=ocr_result,\n",
    "                output_path=viz_path,\n",
    "            )\n",
    "            viz_paths.append(viz_path)\n",
    "            logger.debug(f\"   ðŸ–ï¸  Saved viz: page {i+1}\")\n",
    "\n",
    "        logger.info(f\"âœ… All OCR visualizations saved to: {viz_dir}\")\n",
    "        return viz_paths\n",
    "\n",
    "    # ================================================================== #\n",
    "    #  Step 5: LLM extraction (text-only input)\n",
    "    # ================================================================== #\n",
    "    def _extract_with_llm(self, full_text: str) -> dict:\n",
    "        logger.info(f\"ðŸ¤– Calling LLM model: {self.llm_model} (text-only mode)\")\n",
    "\n",
    "        config = types.GenerateContentConfig(\n",
    "            system_instruction=self.llm_system_prompt,\n",
    "            temperature=self.llm_temperature,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_json_schema=self.llm_output_schema.model_json_schema(),\n",
    "        )\n",
    "\n",
    "        response = self.llm_client.models.generate_content(\n",
    "            model=self.llm_model,\n",
    "            config=config,\n",
    "            contents=f\"Now extract the following OCR'd text:\\n\\n{full_text}\",\n",
    "        )\n",
    "\n",
    "        parsed = self.llm_output_schema.model_validate_json(response.text)\n",
    "        logger.info(\"ðŸŽ¯ LLM extraction completed successfully\")\n",
    "        return parsed.model_dump()\n",
    "\n",
    "    # ================================================================== #\n",
    "    #  Step 6: Save extraction results\n",
    "    # ================================================================== #\n",
    "    def _save_extraction(self, data: dict, run_dir: str, label: str) -> str:\n",
    "        out_path = os.path.join(run_dir, f\"{label}_extraction.json\")\n",
    "        with open(out_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"ðŸ’¾ Saved extraction: {out_path}\")\n",
    "        return out_path\n",
    "\n",
    "    # ================================================================== #\n",
    "    #  Step 7: Extract a single AD PDF (full OCR pipeline)\n",
    "    # ================================================================== #\n",
    "    def extract_ad(\n",
    "        self, pdf_path: str, label: Optional[str] = None\n",
    "    ) -> tuple[dict, list[str], list[dict]]:\n",
    "        \"\"\"\n",
    "        Full OCR extraction pipeline for a single AD PDF.\n",
    "\n",
    "        Returns:\n",
    "            (extraction_dict, img_paths, ocr_results)\n",
    "        \"\"\"\n",
    "        if label is None:\n",
    "            label = self._label_from_path(pdf_path)\n",
    "\n",
    "        run_id = uuid4().hex\n",
    "        run_dir = os.path.join(self.temp_dir, run_id)\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "        self._run_dirs.append(run_dir)\n",
    "        logger.info(f\"ðŸš€ [{label}] Starting OCR extraction â€” run_id={run_id}\")\n",
    "\n",
    "        # PDF -> Images\n",
    "        img_paths = self._pdf_to_images(pdf_path, run_dir)\n",
    "\n",
    "        # Images -> OCR\n",
    "        ocr_results = self._run_ocr(img_paths)\n",
    "\n",
    "        # OCR -> Sorted full text\n",
    "        full_text = self._get_full_text(ocr_results)\n",
    "        logger.info(f\"ðŸ“ Full text extracted: {len(full_text)} characters\")\n",
    "\n",
    "        # Save raw OCR text for debugging\n",
    "        text_path = os.path.join(run_dir, f\"{label}_ocr_text.txt\")\n",
    "        with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(full_text)\n",
    "        logger.debug(f\"   ðŸ“„ Raw OCR text saved: {text_path}\")\n",
    "\n",
    "        # Text -> LLM structured extraction\n",
    "        extraction = self._extract_with_llm(full_text)\n",
    "        self._save_extraction(extraction, run_dir, label)\n",
    "\n",
    "        logger.info(f\"âœ… [{label}] OCR extraction complete!\")\n",
    "        return extraction, img_paths, ocr_results\n",
    "\n",
    "    # ================================================================== #\n",
    "    #  Step 8: Full pipeline\n",
    "    # ================================================================== #\n",
    "    def run_analysis(\n",
    "        self,\n",
    "        test_data_path: str,\n",
    "        ad_file_paths: list[str],\n",
    "        save_dir: str,\n",
    "        cleanup: bool = True,\n",
    "    ) -> str:\n",
    "        logger.info(\"ðŸ”°\" + \"=\" * 58)\n",
    "        logger.info(f\"ðŸ›« Starting AD Recognition Pipeline (OCR) â€” {len(ad_file_paths)} AD(s)\")\n",
    "        logger.info(\"ðŸ”°\" + \"=\" * 58)\n",
    "\n",
    "        try:\n",
    "            # --- Extract all AD PDFs via OCR ---\n",
    "            ad_extractions: dict[str, dict] = {}\n",
    "            ad_ocr_data: dict[str, tuple[list[str], list[dict]]] = {}\n",
    "\n",
    "            for i, pdf_path in enumerate(ad_file_paths, 1):\n",
    "                label = self._label_from_path(pdf_path)\n",
    "                logger.info(f\"ðŸ“‹ [{i}/{len(ad_file_paths)}] Processing: {label}\")\n",
    "                extraction, img_paths, ocr_results = self.extract_ad(pdf_path, label=label)\n",
    "                ad_extractions[label] = extraction\n",
    "                ad_ocr_data[label] = (img_paths, ocr_results)\n",
    "\n",
    "            # --- Save OCR visualizations to save_dir ---\n",
    "\n",
    "            run_timestamp = datetime.now().strftime(\"%y%m%d\")\n",
    "            run_id = uuid4().hex[:8]\n",
    "            run_output_dir = os.path.join(save_dir, f\"{run_id}_{run_timestamp}\")\n",
    "            os.makedirs(run_output_dir, exist_ok=True)\n",
    "            logger.info(f\"ðŸ“ Run output directory: {run_output_dir}\")\n",
    "\n",
    "            # --- Save OCR visualizations ---\n",
    "            if self.save_ocr_viz:\n",
    "                for label, (img_paths, ocr_results) in ad_ocr_data.items():\n",
    "                    self._save_ocr_visualizations(\n",
    "                        img_paths, ocr_results, run_output_dir, label\n",
    "                    )\n",
    "\n",
    "            # --- Load test data ---\n",
    "            logger.info(f\"ðŸ“Š Loading test data: {test_data_path}\")\n",
    "            test_data = pd.read_csv(test_data_path, sep=\",\")\n",
    "            logger.info(f\"ðŸ“ Test data shape: {test_data.shape}\")\n",
    "\n",
    "            # --- Compare ---\n",
    "            logger.info(f\"âš™ï¸  Running AD comparison against {len(ad_extractions)} AD(s)...\")\n",
    "            result_df = compare_to_ad(test_data, ad_file_dict=ad_extractions)\n",
    "            logger.info(f\"ðŸ Comparison done â€” {len(result_df)} rows classified\")\n",
    "\n",
    "            # --- Present results ---\n",
    "            print(\"========== RESULT ==========\")\n",
    "            print(result_df.to_markdown(index=False))\n",
    "            print(\"============================\")\n",
    "\n",
    "            # --- Save results ---\n",
    "            result_path = os.path.join(run_output_dir, \"ad_classification_results.csv\")\n",
    "            result_df.to_csv(result_path, index=False)\n",
    "            logger.info(f\"ðŸ’¾ Results saved: {result_path}\")\n",
    "\n",
    "            extractions_path = os.path.join(run_output_dir, \"ad_extractions.json\")\n",
    "            with open(extractions_path, \"w\") as f:\n",
    "                json.dump(ad_extractions, f, indent=2, ensure_ascii=False)\n",
    "            logger.info(f\"ðŸ’¾ Extractions saved: {extractions_path}\")\n",
    "\n",
    "        finally:\n",
    "            if cleanup:\n",
    "                self._cleanup_temp()\n",
    "\n",
    "        logger.info(\"ðŸ”°\" + \"=\" * 58)\n",
    "        logger.info(\"ðŸŽ‰ Pipeline complete!\")\n",
    "        logger.info(\"ðŸ”°\" + \"=\" * 58)\n",
    "\n",
    "        return result_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a13c2",
   "metadata": {},
   "source": [
    "## Pipeline Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c17809",
   "metadata": {},
   "source": [
    "### LLM ONLY SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a0c302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DPI = 300\n",
    "LLM_MODEL = \"gemini-2.5-flash\"\n",
    "LLM_TEMPERATURE = 0.1\n",
    "\n",
    "# Initializing pipeline\n",
    "pipeline = ADRecognitionFullLLM(\n",
    "    dpi=DPI,\n",
    "    llm_model=LLM_MODEL,\n",
    "    llm_system_prompt=SYSTEM_PROMPT,\n",
    "    llm_temperature=LLM_TEMPERATURE,\n",
    "    llm_output_schema=ADApplicabilityExtraction,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-21 05:24:21.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mðŸ”°==========================================================\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:21.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m193\u001b[0m - \u001b[1mðŸ›« Starting AD Recognition Pipeline â€” 2 AD(s)\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:21.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m194\u001b[0m - \u001b[1mðŸ”°==========================================================\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:21.860\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mðŸ“‹ [1/2] Processing: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:21.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m160\u001b[0m - \u001b[1mðŸš€ [EASA_AD_US-2025-23-53_1] Starting extraction â€” run_id=d606e55fd0db40dca876850496a67dd3\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:21.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_pdf_to_images\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mðŸ“„ Converting PDF to images: /home/naufal/soji_ai/documents/EASA_AD_US-2025-23-53_1.pdf (dpi=300)\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:26.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_pdf_to_images\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mðŸ–¼ï¸  Generated 7 page images\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:26.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m101\u001b[0m - \u001b[1mðŸ“¦ Preparing 7 images for LLM...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:26.383\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1m   ðŸ”— Encoding: 9285d925-2160-4be6-8f55-33a6a6af3d11-1.png\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:26.386\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1m   ðŸ”— Encoding: 9285d925-2160-4be6-8f55-33a6a6af3d11-2.png\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:26.387\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1m   ðŸ”— Encoding: 9285d925-2160-4be6-8f55-33a6a6af3d11-3.png\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:26.388\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1m   ðŸ”— Encoding: 9285d925-2160-4be6-8f55-33a6a6af3d11-4.png\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:26.388\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1m   ðŸ”— Encoding: 9285d925-2160-4be6-8f55-33a6a6af3d11-5.png\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:26.389\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1m   ðŸ”— Encoding: 9285d925-2160-4be6-8f55-33a6a6af3d11-6.png\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:26.390\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1m   ðŸ”— Encoding: 9285d925-2160-4be6-8f55-33a6a6af3d11-7.png\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:26.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m113\u001b[0m - \u001b[1mâœ… All images encoded and ready\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:26.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_extract_with_llm\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mðŸ¤– Calling LLM model: gemini-2.5-flash\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:54.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_extract_with_llm\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mðŸŽ¯ LLM extraction completed successfully\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:54.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_extraction\u001b[0m:\u001b[36m146\u001b[0m - \u001b[1mðŸ’¾ Saved extraction: /home/naufal/soji_ai/notebooks/tmp/ad_recognition/d606e55fd0db40dca876850496a67dd3/EASA_AD_US-2025-23-53_1_extraction.json\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:54.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mâœ… [EASA_AD_US-2025-23-53_1] Extraction complete!\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:54.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m201\u001b[0m - \u001b[1mðŸ“‹ [2/2] Processing: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:54.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m160\u001b[0m - \u001b[1mðŸš€ [EASA_AD_2025-0254R1_1] Starting extraction â€” run_id=c3ff3f6077694833b5e9d80d2c6acef4\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:54.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_pdf_to_images\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mðŸ“„ Converting PDF to images: /home/naufal/soji_ai/documents/EASA_AD_2025-0254R1_1.pdf (dpi=300)\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:58.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_pdf_to_images\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mðŸ–¼ï¸  Generated 5 page images\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:58.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m101\u001b[0m - \u001b[1mðŸ“¦ Preparing 5 images for LLM...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:58.120\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1m   ðŸ”— Encoding: 167d23e4-0587-4972-835f-600b4ea8e39d-1.png\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:58.121\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1m   ðŸ”— Encoding: 167d23e4-0587-4972-835f-600b4ea8e39d-2.png\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:58.121\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1m   ðŸ”— Encoding: 167d23e4-0587-4972-835f-600b4ea8e39d-3.png\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:58.122\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1m   ðŸ”— Encoding: 167d23e4-0587-4972-835f-600b4ea8e39d-4.png\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:58.123\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1m   ðŸ”— Encoding: 167d23e4-0587-4972-835f-600b4ea8e39d-5.png\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:58.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_prepare_messages\u001b[0m:\u001b[36m113\u001b[0m - \u001b[1mâœ… All images encoded and ready\u001b[0m\n",
      "\u001b[32m2026-02-21 05:24:58.124\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_extract_with_llm\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mðŸ¤– Calling LLM model: gemini-2.5-flash\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_extract_with_llm\u001b[0m:\u001b[36m136\u001b[0m - \u001b[1mðŸŽ¯ LLM extraction completed successfully\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_extraction\u001b[0m:\u001b[36m146\u001b[0m - \u001b[1mðŸ’¾ Saved extraction: /home/naufal/soji_ai/notebooks/tmp/ad_recognition/c3ff3f6077694833b5e9d80d2c6acef4/EASA_AD_2025-0254R1_1_extraction.json\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m167\u001b[0m - \u001b[1mâœ… [EASA_AD_2025-0254R1_1] Extraction complete!\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1mðŸ“Š Loading test data: /home/naufal/soji_ai/test/ad_test_data.csv\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m208\u001b[0m - \u001b[1mðŸ“ Test data shape: (10, 3)\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.789\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mâš™ï¸  Running AD comparison against 2 AD(s)...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: MD-11, MSN: 48123, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.792\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: DC-10-30F, MSN: 47890, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.792\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.793\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: Boeing 737-800, MSN: 30123, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.794\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.794\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A320-214, MSN: 5234, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.795\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.796\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A320-232, MSN: 6789, mods: ['mod 24591 (production)']\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.797\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.797\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A320-214, MSN: 7456, mods: ['SB A320-57-1089 Rev 04']\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.798\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.798\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A321-111, MSN: 8123, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.799\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.799\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A321-112, MSN: 364, mods: ['mod 24977 (production)']\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.801\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.801\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A319-100, MSN: 9234, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.805\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.805\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: MD-10-10F, MSN: 46234, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m213\u001b[0m - \u001b[1mðŸ Comparison done â€” 10 rows classified\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m225\u001b[0m - \u001b[1mðŸ“ Run output directory: /home/naufal/soji_ai/results/317003c2_260221\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m229\u001b[0m - \u001b[1mðŸ’¾ Results saved: /home/naufal/soji_ai/results/317003c2_260221/ad_classification_results.csv\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mðŸ’¾ Extractions saved: /home/naufal/soji_ai/results/317003c2_260221/ad_extractions.json\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mðŸ§¹ Cleaning up 2 temp directories...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.826\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m63\u001b[0m - \u001b[34m\u001b[1m   ðŸ—‘ï¸  Removed: /home/naufal/soji_ai/notebooks/tmp/ad_recognition/d606e55fd0db40dca876850496a67dd3\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.828\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m63\u001b[0m - \u001b[34m\u001b[1m   ðŸ—‘ï¸  Removed: /home/naufal/soji_ai/notebooks/tmp/ad_recognition/c3ff3f6077694833b5e9d80d2c6acef4\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.830\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m72\u001b[0m - \u001b[34m\u001b[1m   ðŸ—‘ï¸  Removed empty temp dir: /home/naufal/soji_ai/notebooks/tmp/ad_recognition\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mâœ… Cleanup complete\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.831\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m240\u001b[0m - \u001b[1mðŸ”°==========================================================\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m241\u001b[0m - \u001b[1mðŸŽ‰ Pipeline complete!\u001b[0m\n",
      "\u001b[32m2026-02-21 05:25:40.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m242\u001b[0m - \u001b[1mðŸ”°==========================================================\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== RESULT ==========\n",
      "| aircraft_model   |   msn | modifications_applied   | EASA_AD_US-2025-23-53_1   | EASA_AD_2025-0254R1_1   |\n",
      "|:-----------------|------:|:------------------------|:--------------------------|:------------------------|\n",
      "| MD-11            | 48123 | nan                     | âœ… Affected               | âŒ Not applicable       |\n",
      "| DC-10-30F        | 47890 | nan                     | âœ… Affected               | âŒ Not applicable       |\n",
      "| Boeing 737-800   | 30123 | nan                     | âŒ Not applicable         | âŒ Not applicable       |\n",
      "| A320-214         |  5234 | nan                     | âŒ Not applicable         | âœ… Affected             |\n",
      "| A320-232         |  6789 | mod 24591 (production)  | âŒ Not applicable         | âŒ Not Affected         |\n",
      "| A320-214         |  7456 | SB A320-57-1089 Rev 04  | âŒ Not applicable         | âŒ Not Affected         |\n",
      "| A321-111         |  8123 | nan                     | âŒ Not applicable         | âœ… Affected             |\n",
      "| A321-112         |   364 | mod 24977 (production)  | âŒ Not applicable         | âŒ Not Affected         |\n",
      "| A319-100         |  9234 | nan                     | âŒ Not applicable         | âŒ Not applicable       |\n",
      "| MD-10-10F        | 46234 | nan                     | âœ… Affected               | âŒ Not applicable       |\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "current_path = Path(os.getcwd()) # Notebook Path\n",
    "root_project_path = current_path.parent\n",
    "\n",
    "TEST_DATA_PATH = os.path.join(root_project_path, \"test/ad_test_data.csv\") # CHANGE THIS BASED ON YOUR PATH\n",
    "AD_FILE_DIR = os.path.join(root_project_path, \"documents\") # CHANGE THIS BASED ON YOUR PATH\n",
    "AD_FILE_PATHS = [os.path.join(AD_FILE_DIR, file_path) for file_path in os.listdir(AD_FILE_DIR)]\n",
    "SAVE_DIR = os.path.join(root_project_path, \"results\") # CHANGE THIS BASED ON YOUR PATH\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "CLEANUP = True\n",
    "\n",
    "# Run Analysis\n",
    "result_path = pipeline.run_analysis(\n",
    "    test_data_path=TEST_DATA_PATH,\n",
    "    ad_file_paths=AD_FILE_PATHS,\n",
    "    save_dir=SAVE_DIR,\n",
    "    cleanup=CLEANUP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1ae41",
   "metadata": {},
   "source": [
    "### LOCAL OCR + LLM (TEXT ONLY) CPU ONLY SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c9d7d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-21 05:34:26.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m296\u001b[0m - \u001b[1mðŸ”§ Initializing PaddleOCR engine on CPU with 8 threads...\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv5_mobile_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/naufal/.paddlex/official_models/PP-OCRv5_mobile_det`.\u001b[0m\n",
      "/home/naufal/soji_ai/.venv/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:712: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/naufal/.paddlex/official_models/PP-OCRv5_mobile_rec`.\u001b[0m\n",
      "\u001b[32m2026-02-21 05:34:32.062\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mâœ… PaddleOCR engine ready (CPU mode â€” 8 threads, mkldnn=off, fp32)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "DPI = 300\n",
    "LLM_MODEL = \"gemini-2.5-flash\"\n",
    "LLM_TEMPERATURE = 0.1\n",
    "OCR_DEVICE = \"cpu\" # DO NOT CHANGE THIS\n",
    "OCR_PRECISION = \"fp32\" # DO NOT CHANGE THIS\n",
    "OCR_DET_MODEL = \"PP-OCRv5_mobile_det\" # DO NOT CHANGE THIS\n",
    "OCR_REC_MODEL = \"PP-OCRv5_mobile_rec\" # DO NOT CHANGE THIS\n",
    "OCR_CPU_THREADS = 8 # ADJUST BASED ON NUMBER OF CPU THREADS\n",
    "OCR_Y_THRESHOLD = 15.0 # RECOMMENDED 10-15\n",
    "OCR_SAVE_VIZ = True # RECOMMENDED TO SAVE\n",
    "\n",
    "# Initializing pipeline\n",
    "pipeline = ADRecognitionOCR(\n",
    "    dpi=DPI,\n",
    "    llm_model=LLM_MODEL,\n",
    "    llm_system_prompt=SYSTEM_PROMPT,\n",
    "    llm_temperature=LLM_TEMPERATURE,\n",
    "    llm_output_schema=ADApplicabilityExtraction,\n",
    "    ocr_device=OCR_DEVICE,\n",
    "    ocr_precision=OCR_PRECISION,\n",
    "    ocr_det_model=OCR_DET_MODEL,\n",
    "    ocr_rec_model=OCR_REC_MODEL,\n",
    "    y_threshold=OCR_Y_THRESHOLD,\n",
    "    save_ocr_viz=OCR_SAVE_VIZ,\n",
    "    cpu_threads=OCR_CPU_THREADS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc26b4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-21 05:34:49.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m644\u001b[0m - \u001b[1mðŸ”°==========================================================\u001b[0m\n",
      "\u001b[32m2026-02-21 05:34:49.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m645\u001b[0m - \u001b[1mðŸ›« Starting AD Recognition Pipeline (OCR) â€” 2 AD(s)\u001b[0m\n",
      "\u001b[32m2026-02-21 05:34:49.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m646\u001b[0m - \u001b[1mðŸ”°==========================================================\u001b[0m\n",
      "\u001b[32m2026-02-21 05:34:49.800\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m655\u001b[0m - \u001b[1mðŸ“‹ [1/2] Processing: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:34:49.801\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m609\u001b[0m - \u001b[1mðŸš€ [EASA_AD_US-2025-23-53_1] Starting OCR extraction â€” run_id=7d2b1090b8f443589f5905c6079152bb\u001b[0m\n",
      "\u001b[32m2026-02-21 05:34:49.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_pdf_to_images\u001b[0m:\u001b[36m363\u001b[0m - \u001b[1mðŸ“„ Converting PDF to images: /home/naufal/soji_ai/documents/EASA_AD_US-2025-23-53_1.pdf (dpi=300)\u001b[0m\n",
      "\u001b[32m2026-02-21 05:34:54.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_pdf_to_images\u001b[0m:\u001b[36m375\u001b[0m - \u001b[1mðŸ–¼ï¸  Generated 7 page images\u001b[0m\n",
      "\u001b[32m2026-02-21 05:34:54.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_run_ocr\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mðŸ” Running OCR on 7 pages...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:36:49.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_run_ocr\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1mâœ… OCR complete â€” 7 pages processed\u001b[0m\n",
      "\u001b[32m2026-02-21 05:36:49.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m619\u001b[0m - \u001b[1mðŸ“ Full text extracted: 16646 characters\u001b[0m\n",
      "\u001b[32m2026-02-21 05:36:49.364\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m625\u001b[0m - \u001b[34m\u001b[1m   ðŸ“„ Raw OCR text saved: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr/7d2b1090b8f443589f5905c6079152bb/EASA_AD_US-2025-23-53_1_ocr_text.txt\u001b[0m\n",
      "\u001b[32m2026-02-21 05:36:49.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_extract_with_llm\u001b[0m:\u001b[36m561\u001b[0m - \u001b[1mðŸ¤– Calling LLM model: gemini-2.5-flash (text-only mode)\u001b[0m\n",
      "[2026-02-21 05:36:49,373] [    INFO] models.py:5613 - AFC is enabled with max remote calls: 10.\n",
      "[2026-02-21 05:37:09,373] [    INFO] _client.py:1025 - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-02-21 05:37:09.389\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_extract_with_llm\u001b[0m:\u001b[36m577\u001b[0m - \u001b[1mðŸŽ¯ LLM extraction completed successfully\u001b[0m\n",
      "\u001b[32m2026-02-21 05:37:09.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_extraction\u001b[0m:\u001b[36m587\u001b[0m - \u001b[1mðŸ’¾ Saved extraction: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr/7d2b1090b8f443589f5905c6079152bb/EASA_AD_US-2025-23-53_1_extraction.json\u001b[0m\n",
      "\u001b[32m2026-02-21 05:37:09.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mâœ… [EASA_AD_US-2025-23-53_1] OCR extraction complete!\u001b[0m\n",
      "\u001b[32m2026-02-21 05:37:09.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m655\u001b[0m - \u001b[1mðŸ“‹ [2/2] Processing: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:37:09.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m609\u001b[0m - \u001b[1mðŸš€ [EASA_AD_2025-0254R1_1] Starting OCR extraction â€” run_id=86f3d328493948b4b6cdd6a04e871b1a\u001b[0m\n",
      "\u001b[32m2026-02-21 05:37:09.395\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_pdf_to_images\u001b[0m:\u001b[36m363\u001b[0m - \u001b[1mðŸ“„ Converting PDF to images: /home/naufal/soji_ai/documents/EASA_AD_2025-0254R1_1.pdf (dpi=300)\u001b[0m\n",
      "\u001b[32m2026-02-21 05:37:12.807\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_pdf_to_images\u001b[0m:\u001b[36m375\u001b[0m - \u001b[1mðŸ–¼ï¸  Generated 5 page images\u001b[0m\n",
      "\u001b[32m2026-02-21 05:37:12.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_run_ocr\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mðŸ” Running OCR on 5 pages...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:38:42.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_run_ocr\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1mâœ… OCR complete â€” 5 pages processed\u001b[0m\n",
      "\u001b[32m2026-02-21 05:38:42.422\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m619\u001b[0m - \u001b[1mðŸ“ Full text extracted: 13529 characters\u001b[0m\n",
      "\u001b[32m2026-02-21 05:38:42.423\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m625\u001b[0m - \u001b[34m\u001b[1m   ðŸ“„ Raw OCR text saved: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr/86f3d328493948b4b6cdd6a04e871b1a/EASA_AD_2025-0254R1_1_ocr_text.txt\u001b[0m\n",
      "\u001b[32m2026-02-21 05:38:42.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_extract_with_llm\u001b[0m:\u001b[36m561\u001b[0m - \u001b[1mðŸ¤– Calling LLM model: gemini-2.5-flash (text-only mode)\u001b[0m\n",
      "[2026-02-21 05:38:42,434] [    INFO] models.py:5613 - AFC is enabled with max remote calls: 10.\n",
      "[2026-02-21 05:39:25,182] [    INFO] _client.py:1025 - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-02-21 05:39:25.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_extract_with_llm\u001b[0m:\u001b[36m577\u001b[0m - \u001b[1mðŸŽ¯ LLM extraction completed successfully\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:25.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_extraction\u001b[0m:\u001b[36m587\u001b[0m - \u001b[1mðŸ’¾ Saved extraction: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr/86f3d328493948b4b6cdd6a04e871b1a/EASA_AD_2025-0254R1_1_extraction.json\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:25.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mâœ… [EASA_AD_2025-0254R1_1] OCR extraction complete!\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:25.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m666\u001b[0m - \u001b[1mðŸ“ Run output directory: /home/naufal/soji_ai/results/c20d7add_260221\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:25.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m543\u001b[0m - \u001b[1mðŸŽ¨ Drawing OCR visualizations for 7 pages...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:25.434\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:25.663\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 2\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:25.888\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 3\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:26.113\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 4\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:26.320\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 5\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:26.534\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 6\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:26.736\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 7\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:26.737\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m554\u001b[0m - \u001b[1mâœ… All OCR visualizations saved to: /home/naufal/soji_ai/results/c20d7add_260221/EASA_AD_US-2025-23-53_1_ocr_viz\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:26.740\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m543\u001b[0m - \u001b[1mðŸŽ¨ Drawing OCR visualizations for 5 pages...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:26.966\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.181\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 2\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.402\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 3\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.618\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 4\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.812\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 5\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m554\u001b[0m - \u001b[1mâœ… All OCR visualizations saved to: /home/naufal/soji_ai/results/c20d7add_260221/EASA_AD_2025-0254R1_1_ocr_viz\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m676\u001b[0m - \u001b[1mðŸ“Š Loading test data: /home/naufal/soji_ai/test/ad_test_data.csv\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m678\u001b[0m - \u001b[1mðŸ“ Test data shape: (10, 3)\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.816\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m681\u001b[0m - \u001b[1mâš™ï¸  Running AD comparison against 2 AD(s)...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: MD-11, MSN: 48123, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.817\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.818\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: DC-10-30F, MSN: 47890, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.819\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.819\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.820\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: Boeing 737-800, MSN: 30123, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.820\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.820\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.821\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A320-214, MSN: 5234, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.821\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.821\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A320-232, MSN: 6789, mods: ['mod 24591 (production)']\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.823\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.823\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A320-214, MSN: 7456, mods: ['SB A320-57-1089 Rev 04']\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.824\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.825\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A321-111, MSN: 8123, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.825\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.826\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.826\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A321-112, MSN: 364, mods: ['mod 24977 (production)']\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.827\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.827\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A319-100, MSN: 9234, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.828\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.828\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: MD-10-10F, MSN: 46234, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.828\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.829\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m683\u001b[0m - \u001b[1mðŸ Comparison done â€” 10 rows classified\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.833\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m693\u001b[0m - \u001b[1mðŸ’¾ Results saved: /home/naufal/soji_ai/results/c20d7add_260221/ad_classification_results.csv\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m698\u001b[0m - \u001b[1mðŸ’¾ Extractions saved: /home/naufal/soji_ai/results/c20d7add_260221/ad_extractions.json\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m341\u001b[0m - \u001b[1mðŸ§¹ Cleaning up 2 temp directories...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.837\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m345\u001b[0m - \u001b[34m\u001b[1m   ðŸ—‘ï¸  Removed: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr/7d2b1090b8f443589f5905c6079152bb\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.838\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m345\u001b[0m - \u001b[34m\u001b[1m   ðŸ—‘ï¸  Removed: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr/86f3d328493948b4b6cdd6a04e871b1a\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.839\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m353\u001b[0m - \u001b[34m\u001b[1m   ðŸ—‘ï¸  Removed empty temp dir: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mâœ… Cleanup complete\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m704\u001b[0m - \u001b[1mðŸ”°==========================================================\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1mðŸŽ‰ Pipeline complete!\u001b[0m\n",
      "\u001b[32m2026-02-21 05:39:27.841\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m706\u001b[0m - \u001b[1mðŸ”°==========================================================\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== RESULT ==========\n",
      "| aircraft_model   |   msn | modifications_applied   | EASA_AD_US-2025-23-53_1   | EASA_AD_2025-0254R1_1   |\n",
      "|:-----------------|------:|:------------------------|:--------------------------|:------------------------|\n",
      "| MD-11            | 48123 | nan                     | âœ… Affected               | âŒ Not applicable       |\n",
      "| DC-10-30F        | 47890 | nan                     | âœ… Affected               | âŒ Not applicable       |\n",
      "| Boeing 737-800   | 30123 | nan                     | âŒ Not applicable         | âŒ Not applicable       |\n",
      "| A320-214         |  5234 | nan                     | âŒ Not applicable         | âœ… Affected             |\n",
      "| A320-232         |  6789 | mod 24591 (production)  | âŒ Not applicable         | âŒ Not Affected         |\n",
      "| A320-214         |  7456 | SB A320-57-1089 Rev 04  | âŒ Not applicable         | âŒ Not Affected         |\n",
      "| A321-111         |  8123 | nan                     | âŒ Not applicable         | âœ… Affected             |\n",
      "| A321-112         |   364 | mod 24977 (production)  | âŒ Not applicable         | âŒ Not Affected         |\n",
      "| A319-100         |  9234 | nan                     | âŒ Not applicable         | âŒ Not applicable       |\n",
      "| MD-10-10F        | 46234 | nan                     | âœ… Affected               | âŒ Not applicable       |\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "current_path = Path(os.getcwd()) # Notebook Path\n",
    "root_project_path = current_path.parent\n",
    "\n",
    "TEST_DATA_PATH = os.path.join(root_project_path, \"test/ad_test_data.csv\") # CHANGE THIS BASED ON YOUR PATH\n",
    "AD_FILE_DIR = os.path.join(root_project_path, \"documents\") # CHANGE THIS BASED ON YOUR PATH\n",
    "AD_FILE_PATHS = [os.path.join(AD_FILE_DIR, file_path) for file_path in os.listdir(AD_FILE_DIR)]\n",
    "SAVE_DIR = os.path.join(root_project_path, \"results\") # CHANGE THIS BASED ON YOUR PATH\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "CLEANUP = True\n",
    "\n",
    "# Run Analysis\n",
    "result_path = pipeline.run_analysis(\n",
    "    test_data_path=TEST_DATA_PATH,\n",
    "    ad_file_paths=AD_FILE_PATHS,\n",
    "    save_dir=SAVE_DIR,\n",
    "    cleanup=CLEANUP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab567fa",
   "metadata": {},
   "source": [
    "### LOCAL OCR + LLM (TEXT ONLY) GPU ONLY SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f25e0568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-21 05:55:35.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1mðŸ”§ Initializing PaddleOCR engine on gpu:0...\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv5_mobile_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/naufal/.paddlex/official_models/PP-OCRv5_mobile_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/naufal/.paddlex/official_models/PP-OCRv5_mobile_rec`.\u001b[0m\n",
      "\u001b[32m2026-02-21 05:55:38.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m319\u001b[0m - \u001b[1mâœ… PaddleOCR engine ready (gpu:0, fp16)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "DPI = 300\n",
    "LLM_MODEL = \"gemini-2.5-flash\"\n",
    "LLM_TEMPERATURE = 0.1\n",
    "OCR_DEVICE = \"gpu:0\" # DO NOT CHANGE THIS\n",
    "OCR_PRECISION = \"fp16\" # RECOMMENDED FOR GPU\n",
    "OCR_DET_MODEL = \"PP-OCRv5_mobile_det\" # DO NOT CHANGE THIS\n",
    "OCR_REC_MODEL = \"PP-OCRv5_mobile_rec\" # DO NOT CHANGE THIS\n",
    "OCR_CPU_THREADS = 8 # ADJUST BASED ON NUMBER OF CPU THREADS\n",
    "OCR_Y_THRESHOLD = 15.0 # RECOMMENDED 10-15\n",
    "OCR_SAVE_VIZ = True # RECOMMENDED TO SAVE\n",
    "\n",
    "# Initializing pipeline\n",
    "pipeline = ADRecognitionOCR(\n",
    "    dpi=DPI,\n",
    "    llm_model=LLM_MODEL,\n",
    "    llm_system_prompt=SYSTEM_PROMPT,\n",
    "    llm_temperature=LLM_TEMPERATURE,\n",
    "    llm_output_schema=ADApplicabilityExtraction,\n",
    "    ocr_device=OCR_DEVICE,\n",
    "    ocr_precision=OCR_PRECISION,\n",
    "    ocr_det_model=OCR_DET_MODEL,\n",
    "    ocr_rec_model=OCR_REC_MODEL,\n",
    "    y_threshold=OCR_Y_THRESHOLD,\n",
    "    save_ocr_viz=OCR_SAVE_VIZ,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d6e149f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-02-21 05:56:18.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m644\u001b[0m - \u001b[1mðŸ”°==========================================================\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:18.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m645\u001b[0m - \u001b[1mðŸ›« Starting AD Recognition Pipeline (OCR) â€” 2 AD(s)\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:18.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m646\u001b[0m - \u001b[1mðŸ”°==========================================================\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:18.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m655\u001b[0m - \u001b[1mðŸ“‹ [1/2] Processing: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:18.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m609\u001b[0m - \u001b[1mðŸš€ [EASA_AD_US-2025-23-53_1] Starting OCR extraction â€” run_id=f740392b2f274cc587c5dca134de7a0b\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:18.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_pdf_to_images\u001b[0m:\u001b[36m363\u001b[0m - \u001b[1mðŸ“„ Converting PDF to images: /home/naufal/soji_ai/documents/EASA_AD_US-2025-23-53_1.pdf (dpi=300)\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:22.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_pdf_to_images\u001b[0m:\u001b[36m375\u001b[0m - \u001b[1mðŸ–¼ï¸  Generated 7 page images\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:22.593\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_run_ocr\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mðŸ” Running OCR on 7 pages...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:30.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_run_ocr\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1mâœ… OCR complete â€” 7 pages processed\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:30.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m619\u001b[0m - \u001b[1mðŸ“ Full text extracted: 16646 characters\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:30.704\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m625\u001b[0m - \u001b[34m\u001b[1m   ðŸ“„ Raw OCR text saved: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr/f740392b2f274cc587c5dca134de7a0b/EASA_AD_US-2025-23-53_1_ocr_text.txt\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:30.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_extract_with_llm\u001b[0m:\u001b[36m561\u001b[0m - \u001b[1mðŸ¤– Calling LLM model: gemini-2.5-flash (text-only mode)\u001b[0m\n",
      "[2026-02-21 05:56:30,710] [    INFO] models.py:5613 - AFC is enabled with max remote calls: 10.\n",
      "[2026-02-21 05:56:51,303] [    INFO] _client.py:1025 - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-02-21 05:56:51.324\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_extract_with_llm\u001b[0m:\u001b[36m577\u001b[0m - \u001b[1mðŸŽ¯ LLM extraction completed successfully\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:51.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_extraction\u001b[0m:\u001b[36m587\u001b[0m - \u001b[1mðŸ’¾ Saved extraction: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr/f740392b2f274cc587c5dca134de7a0b/EASA_AD_US-2025-23-53_1_extraction.json\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:51.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mâœ… [EASA_AD_US-2025-23-53_1] OCR extraction complete!\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:51.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m655\u001b[0m - \u001b[1mðŸ“‹ [2/2] Processing: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:51.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m609\u001b[0m - \u001b[1mðŸš€ [EASA_AD_2025-0254R1_1] Starting OCR extraction â€” run_id=aabf752a779d45afb0894d0e6adb46ea\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:51.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_pdf_to_images\u001b[0m:\u001b[36m363\u001b[0m - \u001b[1mðŸ“„ Converting PDF to images: /home/naufal/soji_ai/documents/EASA_AD_2025-0254R1_1.pdf (dpi=300)\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:54.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_pdf_to_images\u001b[0m:\u001b[36m375\u001b[0m - \u001b[1mðŸ–¼ï¸  Generated 5 page images\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:54.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_run_ocr\u001b[0m:\u001b[36m382\u001b[0m - \u001b[1mðŸ” Running OCR on 5 pages...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:59.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_run_ocr\u001b[0m:\u001b[36m384\u001b[0m - \u001b[1mâœ… OCR complete â€” 5 pages processed\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:59.776\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m619\u001b[0m - \u001b[1mðŸ“ Full text extracted: 13529 characters\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:59.776\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m625\u001b[0m - \u001b[34m\u001b[1m   ðŸ“„ Raw OCR text saved: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr/aabf752a779d45afb0894d0e6adb46ea/EASA_AD_2025-0254R1_1_ocr_text.txt\u001b[0m\n",
      "\u001b[32m2026-02-21 05:56:59.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_extract_with_llm\u001b[0m:\u001b[36m561\u001b[0m - \u001b[1mðŸ¤– Calling LLM model: gemini-2.5-flash (text-only mode)\u001b[0m\n",
      "[2026-02-21 05:56:59,781] [    INFO] models.py:5613 - AFC is enabled with max remote calls: 10.\n",
      "[2026-02-21 05:57:37,963] [    INFO] _client.py:1025 - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "\u001b[32m2026-02-21 05:57:37.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_extract_with_llm\u001b[0m:\u001b[36m577\u001b[0m - \u001b[1mðŸŽ¯ LLM extraction completed successfully\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:37.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_extraction\u001b[0m:\u001b[36m587\u001b[0m - \u001b[1mðŸ’¾ Saved extraction: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr/aabf752a779d45afb0894d0e6adb46ea/EASA_AD_2025-0254R1_1_extraction.json\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:37.969\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract_ad\u001b[0m:\u001b[36m631\u001b[0m - \u001b[1mâœ… [EASA_AD_2025-0254R1_1] OCR extraction complete!\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:37.971\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m666\u001b[0m - \u001b[1mðŸ“ Run output directory: /home/naufal/soji_ai/results/34286e39_260221\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:37.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m543\u001b[0m - \u001b[1mðŸŽ¨ Drawing OCR visualizations for 7 pages...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:38.190\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:38.434\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 2\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:38.668\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 3\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:38.899\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 4\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:39.109\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 5\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:39.319\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 6\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:39.521\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 7\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:39.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m554\u001b[0m - \u001b[1mâœ… All OCR visualizations saved to: /home/naufal/soji_ai/results/34286e39_260221/EASA_AD_US-2025-23-53_1_ocr_viz\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:39.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m543\u001b[0m - \u001b[1mðŸŽ¨ Drawing OCR visualizations for 5 pages...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:39.758\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:39.978\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 2\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.206\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 3\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.433\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 4\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.638\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m552\u001b[0m - \u001b[34m\u001b[1m   ðŸ–ï¸  Saved viz: page 5\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save_ocr_visualizations\u001b[0m:\u001b[36m554\u001b[0m - \u001b[1mâœ… All OCR visualizations saved to: /home/naufal/soji_ai/results/34286e39_260221/EASA_AD_2025-0254R1_1_ocr_viz\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.639\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m676\u001b[0m - \u001b[1mðŸ“Š Loading test data: /home/naufal/soji_ai/test/ad_test_data.csv\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.640\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m678\u001b[0m - \u001b[1mðŸ“ Test data shape: (10, 3)\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m681\u001b[0m - \u001b[1mâš™ï¸  Running AD comparison against 2 AD(s)...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.642\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: MD-11, MSN: 48123, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.642\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.643\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: DC-10-30F, MSN: 47890, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.644\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.644\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: Boeing 737-800, MSN: 30123, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.645\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A320-214, MSN: 5234, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.646\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.647\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.647\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A320-232, MSN: 6789, mods: ['mod 24591 (production)']\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.647\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.648\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.649\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A320-214, MSN: 7456, mods: ['SB A320-57-1089 Rev 04']\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.649\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.649\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.650\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A321-111, MSN: 8123, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.650\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.650\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.651\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A321-112, MSN: 364, mods: ['mod 24977 (production)']\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.651\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.652\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.652\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: A319-100, MSN: 9234, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.653\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.653\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m556\u001b[0m - \u001b[1mðŸ”Ž Checking AD status â€” model: MD-10-10F, MSN: 46234, mods: []\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.654\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_US-2025-23-53_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.655\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcompare_to_ad\u001b[0m:\u001b[36m563\u001b[0m - \u001b[34m\u001b[1m   ðŸ“‹ Checking against: EASA_AD_2025-0254R1_1\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m683\u001b[0m - \u001b[1mðŸ Comparison done â€” 10 rows classified\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m693\u001b[0m - \u001b[1mðŸ’¾ Results saved: /home/naufal/soji_ai/results/34286e39_260221/ad_classification_results.csv\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.662\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m698\u001b[0m - \u001b[1mðŸ’¾ Extractions saved: /home/naufal/soji_ai/results/34286e39_260221/ad_extractions.json\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m341\u001b[0m - \u001b[1mðŸ§¹ Cleaning up 2 temp directories...\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.665\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m345\u001b[0m - \u001b[34m\u001b[1m   ðŸ—‘ï¸  Removed: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr/f740392b2f274cc587c5dca134de7a0b\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.666\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m345\u001b[0m - \u001b[34m\u001b[1m   ðŸ—‘ï¸  Removed: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr/aabf752a779d45afb0894d0e6adb46ea\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.666\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m353\u001b[0m - \u001b[34m\u001b[1m   ðŸ—‘ï¸  Removed empty temp dir: /home/naufal/soji_ai/notebooks/tmp/ad_recognition_ocr\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_cleanup_temp\u001b[0m:\u001b[36m357\u001b[0m - \u001b[1mâœ… Cleanup complete\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m704\u001b[0m - \u001b[1mðŸ”°==========================================================\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m705\u001b[0m - \u001b[1mðŸŽ‰ Pipeline complete!\u001b[0m\n",
      "\u001b[32m2026-02-21 05:57:40.670\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrun_analysis\u001b[0m:\u001b[36m706\u001b[0m - \u001b[1mðŸ”°==========================================================\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== RESULT ==========\n",
      "| aircraft_model   |   msn | modifications_applied   | EASA_AD_US-2025-23-53_1   | EASA_AD_2025-0254R1_1   |\n",
      "|:-----------------|------:|:------------------------|:--------------------------|:------------------------|\n",
      "| MD-11            | 48123 | nan                     | âœ… Affected               | âŒ Not applicable       |\n",
      "| DC-10-30F        | 47890 | nan                     | âœ… Affected               | âŒ Not applicable       |\n",
      "| Boeing 737-800   | 30123 | nan                     | âŒ Not applicable         | âŒ Not applicable       |\n",
      "| A320-214         |  5234 | nan                     | âŒ Not applicable         | âœ… Affected             |\n",
      "| A320-232         |  6789 | mod 24591 (production)  | âŒ Not applicable         | âŒ Not Affected         |\n",
      "| A320-214         |  7456 | SB A320-57-1089 Rev 04  | âŒ Not applicable         | âŒ Not Affected         |\n",
      "| A321-111         |  8123 | nan                     | âŒ Not applicable         | âœ… Affected             |\n",
      "| A321-112         |   364 | mod 24977 (production)  | âŒ Not applicable         | âŒ Not Affected         |\n",
      "| A319-100         |  9234 | nan                     | âŒ Not applicable         | âŒ Not applicable       |\n",
      "| MD-10-10F        | 46234 | nan                     | âœ… Affected               | âŒ Not applicable       |\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "current_path = Path(os.getcwd()) # Notebook Path\n",
    "root_project_path = current_path.parent\n",
    "\n",
    "TEST_DATA_PATH = os.path.join(root_project_path, \"test/ad_test_data.csv\") # CHANGE THIS BASED ON YOUR PATH\n",
    "AD_FILE_DIR = os.path.join(root_project_path, \"documents\") # CHANGE THIS BASED ON YOUR PATH\n",
    "AD_FILE_PATHS = [os.path.join(AD_FILE_DIR, file_path) for file_path in os.listdir(AD_FILE_DIR)]\n",
    "SAVE_DIR = os.path.join(root_project_path, \"results\") # CHANGE THIS BASED ON YOUR PATH\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "CLEANUP = True\n",
    "\n",
    "# Run Analysis\n",
    "result_path = pipeline.run_analysis(\n",
    "    test_data_path=TEST_DATA_PATH,\n",
    "    ad_file_paths=AD_FILE_PATHS,\n",
    "    save_dir=SAVE_DIR,\n",
    "    cleanup=CLEANUP,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86279fda",
   "metadata": {},
   "source": [
    "# Written Report: AD Document Extraction System\n",
    "\n",
    "## Approach\n",
    "\n",
    "My pipeline follows a two-stage architecture: **Local OCR (PaddleOCR) â†’ LLM (text-only)** for extraction, followed by a **deterministic rule-based engine** for applicability evaluation.\n",
    "\n",
    "The core idea is straightforward â€” OCR converts the PDF into raw text, the LLM parses that text into structured JSON, and a rule engine checks each aircraft against the parsed data. I chose this over a pure regex/template approach because AD document layouts are not standardized. They vary across issuing authorities, and even within EASA, the formatting shifts between revisions. Any hard-coded parser would need constant patching â€” it's a maintenance trap.\n",
    "\n",
    "I also considered a **full multimodal LLM approach** (sending the PDF directly to a vision-capable model), which would be simpler to implement. I opted against it for two reasons: multimodal inference is more expensive per request, and â€” more critically â€” vision models tend to underperform on small, dense text like MSN lists, modification numbers, and SB identifiers. These are exactly the fields where precision matters most in AD compliance. A missed serial number or a misread modification ID can lead to an incorrect applicability determination, which in aviation is not an acceptable margin of error.\n",
    "\n",
    "The OCR + LLM route gives me a layer of control between the document and the model. I can inspect, clean, and restructure the OCR output before the LLM ever sees it â€” something you simply cannot do when the model is reading the PDF as an image.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "The hardest part was not the LLM extraction itself â€” it was everything around the OCR output.\n",
    "\n",
    "**OCR post-processing** was the most time-consuming challenge. Raw PaddleOCR output is flat and unordered â€” it doesn't inherently understand that a block of text is a table header, or that two columns should be read left-to-right rather than top-to-bottom. I had to build post-processing logic to reconstruct reading order, merge fragmented text blocks, and normalize common OCR artifacts (misread characters in identifiers, inconsistent whitespace, broken line continuations). Getting this right was essential because garbage-in from OCR means garbage-out from the LLM, no matter how good the prompt is.\n",
    "\n",
    "**Schema design and evaluation** was another significant effort. I formulated the extraction output as a strict Pydantic schema (`ADApplicabilityExtraction`) with clearly separated fields for models, MSN constraints, modification constraints, SB constraints, aircraft groups, and required actions. The challenge was handling the many edge cases in AD language â€” \"all MSN except...\", \"whichever occurs first\", recurring vs. one-time compliance, terminating actions that cancel other paragraphs. Each of these required careful schema modeling and explicit extraction rules in the system prompt to prevent the LLM from conflating similar-but-distinct concepts (e.g., Airbus `mod` numbers vs. Service Bulletin identifiers â€” these look similar but have completely different compliance implications).\n",
    "\n",
    "**Building the deterministic applicability engine** required translating nuanced AD logic into boolean checks. The three-stage evaluation (model â†’ MSN â†’ modification/SB exclusion) sounds simple, but the devil is in the details: handling inclusive vs. exclusive range bounds, matching modification identifiers with regex while avoiding partial matches, and correctly implementing exclusion logic where an aircraft is initially in-scope but exempted by an already-embodied mod or SB.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "There are several areas where this approach can fall short:\n",
    "\n",
    "**Layout-dependent information loss.** When OCR flattens a PDF into text, spatial relationships (table structures, column alignments, indentation hierarchies) are partially lost. For most AD paragraphs this is manageable, but for complex multi-column tables â€” like group definitions that map models to MSN ranges â€” the flattened text can be ambiguous. A multimodal model would handle these cases better since it can \"see\" the table structure visually.\n",
    "\n",
    "**LLM extraction is not deterministic.** Even with a strict schema and detailed prompt, the LLM can still occasionally misclassify a field, hallucinate a value, or miss a constraint. This is mitigated by the Pydantic validation layer (malformed output is rejected), but subtle errors â€” like placing a mod number in the SB field â€” can slip through if the prompt guardrails aren't specific enough.\n",
    "\n",
    "**GPU bottleneck in the current setup.** The OCR stage runs on a 4GB VRAM laptop GPU, which is a practical limitation. Processing speed is slower than it would be on a dedicated GPU with more CUDA cores â€” in production, this would need to be addressed with better hardware or a batched processing queue.\n",
    "\n",
    "**With more time, I would:**\n",
    "- Add a confidence scoring layer â€” have the LLM output confidence levels per extracted field, then flag low-confidence extractions for human review.\n",
    "- Build an automated evaluation pipeline that compares LLM extraction output against a ground-truth dataset of manually parsed ADs, measuring field-level precision and recall.\n",
    "- Experiment with a hybrid approach â€” use OCR + LLM as the primary path, but fall back to multimodal extraction for documents where OCR post-processing detects likely table structures that would benefit from visual understanding.\n",
    "\n",
    "## Trade-offs\n",
    "\n",
    "**Why LLM?** Because AD documents are written in natural language with enough variation that deterministic parsing (regex, template matching) is fragile. The LLM absorbs the ambiguity â€” it can handle paraphrased compliance language, varying section orderings, and inconsistent formatting without breaking. The trade-off is cost and non-determinism, but for this use case, the flexibility outweighs the risk, especially when paired with schema validation.\n",
    "\n",
    "**Why OCR + text-only LLM over a full multimodal (VLM) approach?** Three reasons: cost, precision, and control. Text-only inference is cheaper. OCR engines are purpose-built for text recognition and outperform vision models on small/dense identifiers. And the OCR intermediate step gives me a post-processing hook â€” I can clean, validate, and restructure the text before the LLM processes it. With a VLM, the model is a black box between PDF-in and JSON-out; I have no opportunity to intervene when the document is messy.\n",
    "\n",
    "That said, VLMs win on simplicity and layout understanding. If cost were not a constraint and the documents were consistently well-formatted, a multimodal approach would be a perfectly valid â€” and arguably simpler â€” choice. The right answer depends on the production context: how many ADs you're processing, how often, and how much tolerance you have for per-request cost vs. pipeline complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soji_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
